{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1WjaHGHsleVgwSydxWlQAITz4MlZZuONh",
      "authorship_tag": "ABX9TyPd4+caKtwmkebYNx+IGBSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/02_Generate_Embedding_Variants_MultiModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZuKmg6t0TNZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 02_Generate_Embeddings_for_All_Models.ipynb\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Load pre-trained, standard fine-tuned, and advanced fine-tuned retriever models.\n",
        "# 2. Apply instruction prefixes where necessary for a fair comparison.\n",
        "# 3. Generate a comprehensive set of passage embeddings for all models\n",
        "#    and context strategies.\n",
        "# ==============================================================================\n",
        "\n",
        "# !pip install -q -U sentence-transformers transformers networkx\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- Config ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "\n",
        "# --- Model Input Folders ---\n",
        "FINETUNED_RETRIEVER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "ADVANCED_FINETUNED_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers_advanced\")\n",
        "\n",
        "# --- Embedding Output Folder ---\n",
        "OUTPUT_FOLDER = os.path.join(BASE_PATH, \"embeddings_full_comparison\")\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Load Graph ---\n",
        "print(\"Loading graph...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "print(\"Graph loaded successfully.\")\n",
        "\n",
        "# --- Models to Use for Embedding ---\n",
        "MODELS_TO_USE = {\n",
        "    # Advanced Fine-Tuned Models (with hard negatives and instructions)\n",
        "    \"e5-large-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"bge-base-en-v1.5\"),\n",
        "\n",
        "    # Standard Fine-Tuned Models (from basic fine-tuning)\n",
        "    \"e5-large-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"bge-base-en-v1.5\"),\n",
        "\n",
        "    # Pre-trained Original Models\n",
        "    \"e5-large-v2_Pretrained\": \"intfloat/e5-large-v2\",\n",
        "    \"all-mpnet-base-v2_Pretrained\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"bge-base-en-v1.5_Pretrained\": \"BAAI/bge-base-en-v1.5\",\n",
        "}\n",
        "\n",
        "# --- Neighbor Configurations ---\n",
        "neighbor_configs = {\n",
        "    \"passage_only\": lambda G, node: [],\n",
        "    \"parent\": lambda G, node: [p for p in G.predecessors(node) if G.get_edge_data(p, node, {}).get(\"type\") == \"PARENT_OF\"],\n",
        "    \"parent_child\": lambda G, node: \\\n",
        "        [p for p in G.predecessors(node) if G.get_edge_data(p, node, {}).get(\"type\") == \"PARENT_OF\"] + \\\n",
        "        [s for s in G.successors(node) if G.get_edge_data(node, s, {}).get(\"type\") == \"PARENT_OF\"],\n",
        "    \"parent_child_cites\": lambda G, node: \\\n",
        "        list(set( # Use set to avoid duplicates\n",
        "            [p for p in G.predecessors(node) if G.get_edge_data(p, node, {}).get(\"type\") == \"PARENT_OF\"] + \\\n",
        "            [s for s in G.successors(node) if G.get_edge_data(node, s, {}).get(\"type\") in [\"PARENT_OF\", \"CITES\"]]\n",
        "        )),\n",
        "    \"full_neighborhood\": lambda G, node: list(set(list(G.predecessors(node)) + list(G.successors(node))))\n",
        "}\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def add_instruction(text, model_key):\n",
        "    # Apply instructions only to models trained with them (advanced FT and their pre-trained versions)\n",
        "    if \"e5\" in model_key:\n",
        "        return f\"passage: {text}\"\n",
        "    # BGE models don't require a prefix for passages, only for queries (which is handled in the eval script)\n",
        "    return text\n",
        "\n",
        "def build_contextual_text(G, node_id, get_neighbors_func, model_key):\n",
        "    \"\"\"\n",
        "    Constructs the text for embedding, applying instruction prefixes where needed.\n",
        "    \"\"\"\n",
        "    base_text = G.nodes[node_id].get(\"text\", \"\")\n",
        "    # Apply instruction only if the model is an e5 model\n",
        "    instructed_base_text = add_instruction(base_text, model_key)\n",
        "\n",
        "    context_parts = [instructed_base_text]\n",
        "    neighbors = get_neighbors_func(G, node_id)\n",
        "    for neighbor_id in neighbors:\n",
        "        context_text = G.nodes[neighbor_id].get(\"text\", \"\")\n",
        "        if context_text:\n",
        "            instructed_context = add_instruction(context_text, model_key)\n",
        "            context_parts.append(instructed_context)\n",
        "\n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "# --- Run All Combinations ---\n",
        "for model_key, model_path in MODELS_TO_USE.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- Loading Model: {model_key} from {model_path} ---\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        model = SentenceTransformer(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load model {model_key}. Skipping. Error: {e}\")\n",
        "        continue\n",
        "\n",
        "    for config_key, get_neighbors in neighbor_configs.items():\n",
        "        print(f\"\\nüîç Generating embeddings: Model={model_key}, Context={config_key}\")\n",
        "\n",
        "        out_dir = os.path.join(OUTPUT_FOLDER, model_key, config_key)\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        texts_to_encode, uids_to_save = [], []\n",
        "\n",
        "        print(\"Preparing texts from graph nodes...\")\n",
        "        all_passage_nodes = [node_id for node_id, data in G.nodes(data=True) if data.get(\"type\") == \"Passage\"]\n",
        "\n",
        "        for node_id in tqdm(all_passage_nodes, desc=\"Building Context\"):\n",
        "            full_text = build_contextual_text(G, node_id, get_neighbors, model_key)\n",
        "            texts_to_encode.append(full_text)\n",
        "            uids_to_save.append(node_id)\n",
        "\n",
        "        if not texts_to_encode:\n",
        "            print(\"‚ö†Ô∏è Warning: No passages found to embed. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Found {len(texts_to_encode)} passages to embed.\")\n",
        "        embeddings = model.encode(texts_to_encode, show_progress_bar=True, batch_size=32)\n",
        "\n",
        "        # Save the generated embeddings and corresponding UIDs\n",
        "        with open(os.path.join(out_dir, \"passage_ids.json\"), \"w\") as f:\n",
        "            json.dump(uids_to_save, f)\n",
        "        with open(os.path.join(out_dir, \"embeddings.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(embeddings, f)\n",
        "\n",
        "        print(f\"‚úÖ Saved: {out_dir}\")\n",
        "\n",
        "print(\"\\nAll embedding generation tasks are complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "3oQFsL2JIKij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}