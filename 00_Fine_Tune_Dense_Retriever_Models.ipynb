{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN+B2buhsL87Ml1FsF+MFhz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/00_Fine_Tune_Dense_Retriever_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n_ORwrme-3Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vcgTgSz-w-p"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Fine-Tune Dense Retriever Models\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Load a training dataset containing questions and their corresponding\n",
        "#    relevant passages.\n",
        "# 2. For each base sentence-transformer model specified, fine-tune it on the\n",
        "#    training data using Multiple Negatives Ranking Loss.\n",
        "# 3. Use a validation set to evaluate the model during training and save the\n",
        "#    best-performing checkpoint.\n",
        "# 4. Save each fine-tuned model to a designated folder, making them\n",
        "#    ready for re-evaluation in Experiment 1.\n",
        "#\n",
        "# This script should be run in a Google Colab environment with the Drive mounted.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "# This command ensures compatible library versions to prevent import errors.\n",
        "!pip install -q -U sentence-transformers transformers datasets\n",
        "\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "TRAIN_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_train.json\")\n",
        "# Path to the validation (development) set\n",
        "DEV_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_val.json\")\n",
        "# This is the output folder where the new models will be saved\n",
        "MODEL_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "os.makedirs(MODEL_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Models to Fine-Tune ---\n",
        "# A selection of strong base models for dense retrieval\n",
        "BASE_MODELS_TO_FINETUNE = {\n",
        "    \"e5-large-v2\": \"intfloat/e5-large-v2\",\n",
        "    \"all-mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"bge-base-en-v1.5\": \"BAAI/bge-base-en-v1.5\"\n",
        "}\n",
        "\n",
        "# --- Training Parameters ---\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 16 # A larger batch size is beneficial for MultipleNegativesRankingLoss\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# --- Load and Prepare Data ---\n",
        "print(\"Loading training and validation data...\")\n",
        "try:\n",
        "    with open(TRAIN_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        train_data = json.load(f)\n",
        "    with open(DEV_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        dev_data = json.load(f)\n",
        "    print(f\"Loaded {len(train_data)} training and {len(dev_data)} validation examples.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"FATAL ERROR: Data file not found: {e}. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# Convert training data to InputExample format for MultipleNegativesRankingLoss\n",
        "train_samples = []\n",
        "print(\"Preparing training samples...\")\n",
        "for item in tqdm(train_data, desc=\"Processing training data\"):\n",
        "    query = item[\"Question\"]\n",
        "    for p in item[\"Passages\"]:\n",
        "        positive_passage = p[\"Passage\"]\n",
        "        train_samples.append(InputExample(texts=[query, positive_passage]))\n",
        "print(f\"Created {len(train_samples)} positive training pairs.\")\n",
        "\n",
        "# Prepare validation data for the InformationRetrievalEvaluator\n",
        "print(\"Preparing validation data...\")\n",
        "dev_queries = {}\n",
        "dev_corpus = {}\n",
        "dev_relevant_docs = {}\n",
        "for item in tqdm(dev_data, desc=\"Processing validation data\"):\n",
        "    qid = item[\"QuestionID\"]\n",
        "    dev_queries[qid] = item[\"Question\"]\n",
        "    dev_relevant_docs[qid] = set()\n",
        "    for p in item[\"Passages\"]:\n",
        "        # Use a unique passage identifier for the corpus and relevance mapping\n",
        "        # Here we assume PassageID is unique across the entire dataset\n",
        "        pid = p[\"PassageID\"]\n",
        "        dev_corpus[pid] = p[\"Passage\"]\n",
        "        dev_relevant_docs[qid].add(pid)\n",
        "print(\"Validation data prepared.\")\n",
        "\n",
        "\n",
        "# --- Main Fine-Tuning Loop ---\n",
        "for model_name, model_path in BASE_MODELS_TO_FINETUNE.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- Fine-Tuning Model: {model_name} ---\")\n",
        "    print(f\"Base model: {model_path}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Initialize the SentenceTransformer model\n",
        "    model = SentenceTransformer(model_path)\n",
        "\n",
        "    # 2. Create a DataLoader\n",
        "    # The library handles the collation and batching process.\n",
        "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # 3. Define the loss function\n",
        "    # MultipleNegativesRankingLoss is the state-of-the-art for training retrievers.\n",
        "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "\n",
        "    # 4. Define the output path for this model\n",
        "    output_save_path = os.path.join(MODEL_OUTPUT_FOLDER, model_name)\n",
        "\n",
        "    # 5. Create the evaluator\n",
        "    evaluator = InformationRetrievalEvaluator(\n",
        "        queries=dev_queries,\n",
        "        corpus=dev_corpus,\n",
        "        relevant_docs=dev_relevant_docs,\n",
        "        name=f\"{model_name}-val\",\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # 6. Start the training process\n",
        "    model.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        evaluator=evaluator,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        warmup_steps=int(len(train_dataloader) * 0.1), # 10% of steps for warmup\n",
        "        output_path=output_save_path,\n",
        "        show_progress_bar=True,\n",
        "        checkpoint_save_steps=int(len(train_dataloader) * 0.25), # Save a checkpoint 4 times per epoch\n",
        "        checkpoint_path=os.path.join(output_save_path, \"checkpoints\"),\n",
        "        # Save the best model based on its performance on the validation set\n",
        "        save_best_model=True\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Best model for '{model_name}' fine-tuned and saved to: {output_save_path}\")\n",
        "\n",
        "print(\"\\nAll dense retriever models have been fine-tuned successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fM9IyOPb-8Ob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}