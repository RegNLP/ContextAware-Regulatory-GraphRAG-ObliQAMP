{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1vlobT3bmUKnvXzWdGQ6LJfnaE3DYp7lc",
      "authorship_tag": "ABX9TyPs4etgSHsV4uXRs1giOtqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/03_Fine_Tune_Cross_Encoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0_3JJFzlWI_"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Fine-Tune Cross-Encoder Models for Re-ranking (High Quality)\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Load training and validation datasets.\n",
        "# 2. Generate high-quality training data by mining \"hard negatives\" using BM25.\n",
        "# 3. Fine-tune each base cross-encoder model, using the validation set to\n",
        "#    evaluate performance after each epoch and save only the best model.\n",
        "# 4. Save each fine-tuned model to a designated folder for Experiment 3.\n",
        "#\n",
        "# This script should be run in a Google Colab environment with the Drive mounted.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "# This command ensures compatible library versions to prevent import errors.\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import CrossEncoder, InputExample\n",
        "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "TRAIN_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_train.json\")\n",
        "# Assumes you have a dev/validation set in the same folder\n",
        "DEV_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_val.json\")\n",
        "MODEL_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_cross_encoders\")\n",
        "os.makedirs(MODEL_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Models to Fine-Tune ---\n",
        "BASE_MODELS_TO_FINETUNE = {\n",
        "    \"MiniLM_CrossEncoder\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    \"MPNet_CrossEncoder\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"MSMarco_CrossEncoder\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\",\n",
        "    \"BERT_CrossEncoder\": \"bert-base-uncased\"\n",
        "}\n",
        "\n",
        "# --- Training Parameters ---\n",
        "NUM_EPOCHS = 10 # Increased epochs for better convergence with harder negatives\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_HARD_NEGATIVES = 5 # Number of hard negatives to mine for each positive example\n",
        "\n",
        "# --- Load Data ---\n",
        "print(\"Loading training and validation data...\")\n",
        "try:\n",
        "    with open(TRAIN_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        train_data = json.load(f)\n",
        "    with open(DEV_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        dev_data = json.load(f)\n",
        "    print(f\"Loaded {len(train_data)} training and {len(dev_data)} validation examples.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"FATAL ERROR: Data file not found. {e}. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# --- Prepare for Hard Negative Mining ---\n",
        "print(\"Preparing for hard negative mining...\")\n",
        "# Create a corpus of all unique passages from the training data\n",
        "all_passages = {}\n",
        "for item in train_data:\n",
        "    for p in item[\"Passages\"]:\n",
        "        all_passages[p[\"PassageID\"]] = p[\"Passage\"]\n",
        "\n",
        "corpus_pids = list(all_passages.keys())\n",
        "corpus_texts = [all_passages[pid] for pid in corpus_pids]\n",
        "tokenized_corpus = [text.split(\" \") for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 index built for hard negative mining.\")\n",
        "\n",
        "# --- Function to Prepare Samples (with Hard Negative Mining) ---\n",
        "def prepare_samples(data, bm25_model, corpus_pids, corpus, num_negatives):\n",
        "    samples = []\n",
        "    for item in tqdm(data, desc=\"Generating samples\"):\n",
        "        query = item[\"Question\"]\n",
        "        positive_pids = {p[\"PassageID\"] for p in item[\"Passages\"]}\n",
        "\n",
        "        # Create positive examples\n",
        "        for p in item[\"Passages\"]:\n",
        "            samples.append(InputExample(texts=[query, p[\"Passage\"]], label=1.0))\n",
        "\n",
        "        # Mine hard negatives\n",
        "        tokenized_query = query.split(\" \")\n",
        "        bm25_scores = bm25_model.get_scores(tokenized_query)\n",
        "        top_n_indices = np.argsort(bm25_scores)[::-1][:100] # Get top 100 candidates\n",
        "\n",
        "        hard_negatives_added = 0\n",
        "        for idx in top_n_indices:\n",
        "            pid = corpus_pids[idx]\n",
        "            if pid not in positive_pids:\n",
        "                passage_text = corpus[idx]\n",
        "                samples.append(InputExample(texts=[query, passage_text], label=0.0))\n",
        "                hard_negatives_added += 1\n",
        "                if hard_negatives_added >= num_negatives * len(positive_pids):\n",
        "                    break\n",
        "    return samples\n",
        "\n",
        "# --- Create Training and Validation Sets ---\n",
        "train_samples = prepare_samples(train_data, bm25, corpus_pids, corpus_texts, NUM_HARD_NEGATIVES)\n",
        "dev_samples = prepare_samples(dev_data, bm25, corpus_pids, corpus_texts, NUM_HARD_NEGATIVES)\n",
        "\n",
        "print(f\"Created {len(train_samples)} training samples.\")\n",
        "print(f\"Created {len(dev_samples)} validation samples.\")\n",
        "\n",
        "# --- Main Fine-Tuning Loop ---\n",
        "for model_name, model_path in BASE_MODELS_TO_FINETUNE.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- Fine-Tuning Model: {model_name} ---\")\n",
        "    print(f\"Base model: {model_path}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Initialize the CrossEncoder model\n",
        "    model = CrossEncoder(model_path, num_labels=1)\n",
        "\n",
        "    # 2. Create DataLoaders\n",
        "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # 3. Create Evaluator\n",
        "    evaluator = CEBinaryClassificationEvaluator.from_input_examples(dev_samples, name=f\"{model_name}-dev\")\n",
        "\n",
        "    # 4. Define the output path for this model\n",
        "    output_save_path = os.path.join(MODEL_OUTPUT_FOLDER, model_name)\n",
        "\n",
        "    # 5. Start the training process\n",
        "    model.fit(\n",
        "        train_dataloader=train_dataloader,\n",
        "        evaluator=evaluator,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        evaluation_steps=int(len(train_dataloader) * 0.1), # Evaluate every 10% of an epoch\n",
        "        warmup_steps=int(len(train_dataloader) * 0.1),\n",
        "        output_path=output_save_path,\n",
        "        save_best_model=True, # This is crucial for saving the best performing model\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Best model for '{model_name}' fine-tuned and saved to: {output_save_path}\")\n",
        "\n",
        "print(\"\\nAll cross-encoder models have been fine-tuned successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "kYYpPQ3qlqZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}