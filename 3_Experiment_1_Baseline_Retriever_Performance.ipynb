{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1koISL7wv9WHqi4b6D6rMIqlw8SHLb0Nh",
      "authorship_tag": "ABX9TyPJBf3dw+d5qmvq/WiJWTo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/3_Experiment_1_Baseline_Retriever_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1: Baseline Retriever Performance**\n",
        "Goal: Find the best initial retrieval method before any complex re-ranking. This is your foundational baseline.\n",
        "\n",
        "Method:\n",
        "\n",
        "Disable Stage 2 (Graph Re-ranking) and Stage 3 (Cross-Encoder).\n",
        "\n",
        "For each of your 8 embedding sets (e.g., mpnet with passage_only, e5-large with parent, etc.):\n",
        "\n",
        "Run A: Evaluate using only dense retrieval (the sentence transformer).\n",
        "\n",
        "Run B: Evaluate using hybrid retrieval (dense + BM25 fused with RRF).\n",
        "\n",
        "Record the performance (nDCG@10, MAP@10) for all 16 runs (8 embedding sets x 2 methods).\n",
        "\n",
        "Outcome: You'll identify the best-performing embedding model and context strategy. You'll also know if combining it with BM25 provides a significant boost. The winner of this experiment becomes your \"champion retriever\" for the next stage.\n",
        "\n",
        "**Experiment 2: Value of Graph Re-ranking**\n",
        "Goal: Isolate and measure the benefit of using the knowledge graph structure.\n",
        "\n",
        "Method:\n",
        "\n",
        "Use the champion retriever identified in Experiment 1 (e.g., hybrid retrieval with e5-large + parent_child context).\n",
        "\n",
        "Enable Stage 2 (Graph Re-ranking).\n",
        "\n",
        "Keep Stage 3 (Cross-Encoder) disabled.\n",
        "\n",
        "Run the pipeline and evaluate the results.\n",
        "\n",
        "Outcome: By comparing these results to the champion's score from Experiment 1, you can quantify the exact performance change (hopefully an improvement) from the graph-based score bonuses. This tells you if your contextual linking strategy is effective.\n",
        "\n",
        "**Experiment 3: Full Pipeline with Cross-Encoders**\n",
        "Goal: Measure the impact of the final, high-precision re-ranking and find the best overall model combination.\n",
        "\n",
        "Method:\n",
        "\n",
        "Use the best setup from Experiment 2 (Champion Retriever + Graph Re-ranking).\n",
        "\n",
        "Enable Stage 3 (Cross-Encoder Re-ranking).\n",
        "\n",
        "Run the complete, three-stage pipeline for each of your cross-encoder models (MiniLM, MPNet, etc.).\n",
        "\n",
        "Outcome: This will give you the final performance for your complete system. You can compare the different cross-encoders to see which one provides the most significant boost, leading you to the definitive, best-performing pipeline for your task.\n",
        "\n"
      ],
      "metadata": {
        "id": "YS7Vgks29Hdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwC4LrN56B2J",
        "outputId": "8c65593f-43d8-4885-e78b-b3ad1e0b4c12"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To9Y9-H251DG",
        "outputId": "f186411a-b264-4ab7-de0e-b667b94577e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph...\n",
            "Loaded 447 test questions.\n",
            "QREL file found. Skipping generation.\n",
            "Loaded QRELs for 447 queries.\n",
            "Preparing BM25 index...\n",
            "BM25 ready.\n",
            "\n",
            "=== BM25 Retrieval ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BM25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [00:41<00:00, 10.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Final Evaluation Results:\n",
            "Model Context       Method  Recall@10  MAP@10  nDCG@10\n",
            " BM25     N/A Lexical Only     0.3474  0.2611   0.3409\n",
            "\n",
            "âœ… Results saved to: /content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/experiment_1_full_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "'''# ==============================================================================\n",
        "# Experiment 1: Baseline Retriever Evaluation - BM25, Dense, Hybrid\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings\")\n",
        "RESULTS_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_1_full_results.csv\")\n",
        "\n",
        "K = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Embedding Models and Contexts ---\n",
        "EMBEDDING_MODELS = {\n",
        "    \"mpnet\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"e5-large\": \"intfloat/e5-large-v2\"\n",
        "}\n",
        "CONTEXT_CONFIGS = [\"passage_only\", \"parent\", \"parent_child\", \"full_neighborhood\"]\n",
        "\n",
        "# --- Load Graph ---\n",
        "print(\"Loading graph...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "\n",
        "# --- Load Test Set ---\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# --- Generate QREL file if needed ---\n",
        "if not os.path.exists(QREL_PATH):\n",
        "    print(\"QREL file not found. Generating...\")\n",
        "    with open(QREL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            qid = item[\"QuestionID\"]\n",
        "            for passage in item[\"Passages\"]:\n",
        "                doc_id = passage[\"DocumentID\"]\n",
        "                passage_id = passage[\"PassageID\"]\n",
        "                uid = f\"{doc_id}|||{passage_id}\"\n",
        "                f.write(f\"{qid} 0 {uid} 1\\n\")\n",
        "    print(f\"âœ… QREL saved to: {QREL_PATH}\")\n",
        "else:\n",
        "    print(\"QREL file found. Skipping generation.\")\n",
        "\n",
        "# --- Load QRELs ---\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# --- Prepare BM25 ---\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\", \"ndcg_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "# --- Reciprocal Rank Fusion ---\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# --- Store All Results ---\n",
        "all_results = []\n",
        "\n",
        "# --- BM25 Retrieval ---\n",
        "print(\"\\n=== BM25 Retrieval ===\")\n",
        "bm25_run = {}\n",
        "for q in tqdm(test_data, desc=\"BM25\"):\n",
        "    qid = q[\"QuestionID\"]\n",
        "    query = q[\"Question\"]\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    top_idxs = np.argsort(scores)[::-1][:K]\n",
        "    bm25_run[qid] = {}\n",
        "    for idx in top_idxs:\n",
        "        uid = all_passage_uids[idx]\n",
        "        node = G.nodes[uid]\n",
        "        doc_id = node.get(\"document_id\", \"\")\n",
        "        passage_id = node.get(\"passage_id\", \"\")\n",
        "        if doc_id and passage_id:\n",
        "            combined_uid = f\"{doc_id}|||{passage_id}\"\n",
        "            bm25_run[qid][combined_uid] = float(scores[idx])\n",
        "bm25_metrics = evaluate_run(bm25_run, qrel)\n",
        "all_results.append({\n",
        "    \"Model\": \"BM25\", \"Context\": \"N/A\", \"Method\": \"Lexical Only\",\n",
        "    \"Recall@10\": bm25_metrics[\"recall_10\"],\n",
        "    \"MAP@10\": bm25_metrics[\"map_cut_10\"],\n",
        "    \"nDCG@10\": bm25_metrics[\"ndcg_cut_10\"]\n",
        "})\n",
        "# --- Save Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.sort_values(by=[\"Model\", \"Context\", \"Method\"])\n",
        "print(\"\\nðŸ“Š Final Evaluation Results:\")\n",
        "print(df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "df.to_csv(RESULTS_OUTPUT_PATH, index=False)\n",
        "print(f\"\\nâœ… Results saved to: {RESULTS_OUTPUT_PATH}\")'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Experiment 1: Baseline Retriever Evaluation - BM25, Dense, Hybrid\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings\")\n",
        "RESULTS_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_1_full_results.csv\")\n",
        "\n",
        "K = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Embedding Models and Contexts ---\n",
        "EMBEDDING_MODELS = {\n",
        "    \"mpnet\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"e5-large\": \"intfloat/e5-large-v2\"\n",
        "}\n",
        "CONTEXT_CONFIGS = [\"passage_only\", \"parent\", \"parent_child\", \"full_neighborhood\"]\n",
        "\n",
        "# --- Load Graph ---\n",
        "print(\"Loading graph...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "\n",
        "# --- Load Test Set ---\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# --- Generate QREL file if needed ---\n",
        "if not os.path.exists(QREL_PATH):\n",
        "    print(\"QREL file not found. Generating...\")\n",
        "    with open(QREL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            qid = item[\"QuestionID\"]\n",
        "            for passage in item[\"Passages\"]:\n",
        "                doc_id = passage[\"DocumentID\"]\n",
        "                passage_id = passage[\"PassageID\"]\n",
        "                uid = f\"{doc_id}|||{passage_id}\"\n",
        "                f.write(f\"{qid} 0 {uid} 1\\n\")\n",
        "    print(f\"âœ… QREL saved to: {QREL_PATH}\")\n",
        "else:\n",
        "    print(\"QREL file found. Skipping generation.\")\n",
        "\n",
        "# --- Load QRELs ---\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# --- Prepare BM25 ---\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\", \"ndcg_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "# --- Reciprocal Rank Fusion ---\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# --- Store All Results ---\n",
        "all_results = []\n",
        "\n",
        "# --- BM25 Retrieval ---\n",
        "print(\"\\n=== BM25 Retrieval ===\")\n",
        "bm25_run = {}\n",
        "for q in tqdm(test_data, desc=\"BM25\"):\n",
        "    qid = q[\"QuestionID\"]\n",
        "    query = q[\"Question\"]\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    top_idxs = np.argsort(scores)[::-1][:K]\n",
        "    bm25_run[qid] = {}\n",
        "    for idx in top_idxs:\n",
        "        uid = all_passage_uids[idx]\n",
        "        node = G.nodes[uid]\n",
        "        doc_id = node.get(\"document_id\", \"\")\n",
        "        passage_id = node.get(\"passage_id\", \"\")\n",
        "        if doc_id and passage_id:\n",
        "            combined_uid = f\"{doc_id}|||{passage_id}\"\n",
        "            bm25_run[qid][combined_uid] = float(scores[idx])\n",
        "bm25_metrics = evaluate_run(bm25_run, qrel)\n",
        "all_results.append({\n",
        "    \"Model\": \"BM25\", \"Context\": \"N/A\", \"Method\": \"Lexical Only\",\n",
        "    \"Recall@10\": bm25_metrics[\"recall_10\"],\n",
        "    \"MAP@10\": bm25_metrics[\"map_cut_10\"],\n",
        "    \"nDCG@10\": bm25_metrics[\"ndcg_cut_10\"]\n",
        "})\n",
        "\n",
        "# --- Dense + Hybrid Retrieval ---\n",
        "for model_key, model_path in EMBEDDING_MODELS.items():\n",
        "    print(f\"\\n=== Dense Evaluation: {model_key} ===\")\n",
        "    query_encoder = SentenceTransformer(model_path, device=device)\n",
        "\n",
        "    for context_key in CONTEXT_CONFIGS:\n",
        "        print(f\"â†’ Context: {context_key}\")\n",
        "        emb_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"embeddings.pkl\")\n",
        "        id_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"passage_ids.json\")\n",
        "\n",
        "        try:\n",
        "            with open(emb_path, \"rb\") as f:\n",
        "                passage_embeddings = pickle.load(f)\n",
        "            with open(id_path, \"r\") as f:\n",
        "                passage_ids = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âš ï¸ Missing embeddings for {model_key} / {context_key}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        dense_run = {}\n",
        "        hybrid_run = {}\n",
        "\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "\n",
        "        for q in tqdm(test_data, desc=f\"{model_key}/{context_key}\"):\n",
        "            qid = q[\"QuestionID\"]\n",
        "            query = q[\"Question\"]\n",
        "            query_emb = query_encoder.encode(query, convert_to_tensor=True, device=device)\n",
        "            cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "            top_results = torch.topk(cos_scores, k=min(K, len(passage_ids)))\n",
        "\n",
        "            dense_run[qid] = {}\n",
        "            dense_uids = []\n",
        "            for idx, score in zip(top_results.indices, top_results.values):\n",
        "                uid = passage_ids[idx]\n",
        "                node = G.nodes[uid]\n",
        "                doc_id = node.get(\"document_id\", \"\")\n",
        "                passage_id = node.get(\"passage_id\", \"\")\n",
        "                if doc_id and passage_id:\n",
        "                    combined_uid = f\"{doc_id}|||{passage_id}\"\n",
        "                    dense_run[qid][combined_uid] = float(score.item())\n",
        "                    dense_uids.append(uid)\n",
        "\n",
        "            # Hybrid with BM25\n",
        "            tokenized_query = query.split()\n",
        "            bm25_scores = bm25.get_scores(tokenized_query)\n",
        "            top_bm25 = np.argsort(bm25_scores)[::-1][:K]\n",
        "            bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "\n",
        "            fused_uids_with_scores = reciprocal_rank_fusion([dense_uids, bm25_uids])\n",
        "            hybrid_run[qid] = {}\n",
        "            for rank, (uid, score) in enumerate(fused_uids_with_scores[:K]):\n",
        "                node = G.nodes[uid]\n",
        "                doc_id = node.get(\"document_id\", \"\")\n",
        "                passage_id = node.get(\"passage_id\", \"\")\n",
        "                if doc_id and passage_id:\n",
        "                    combined_uid = f\"{doc_id}|||{passage_id}\"\n",
        "                    hybrid_run[qid][combined_uid] = float(score)\n",
        "\n",
        "        # Evaluate Dense\n",
        "        dense_metrics = evaluate_run(dense_run, qrel)\n",
        "        all_results.append({\n",
        "            \"Model\": model_key, \"Context\": context_key, \"Method\": \"Dense\",\n",
        "            \"Recall@10\": dense_metrics[\"recall_10\"],\n",
        "            \"MAP@10\": dense_metrics[\"map_cut_10\"],\n",
        "            \"nDCG@10\": dense_metrics[\"ndcg_cut_10\"]\n",
        "        })\n",
        "\n",
        "        # Evaluate Hybrid\n",
        "        hybrid_metrics = evaluate_run(hybrid_run, qrel)\n",
        "        all_results.append({\n",
        "            \"Model\": model_key, \"Context\": context_key, \"Method\": \"Hybrid\",\n",
        "            \"Recall@10\": hybrid_metrics[\"recall_10\"],\n",
        "            \"MAP@10\": hybrid_metrics[\"map_cut_10\"],\n",
        "            \"nDCG@10\": hybrid_metrics[\"ndcg_cut_10\"]\n",
        "        })\n",
        "\n",
        "# --- Save Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.sort_values(by=[\"Model\", \"Context\", \"Method\"])\n",
        "print(\"\\nðŸ“Š Final Evaluation Results:\")\n",
        "print(df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "df.to_csv(RESULTS_OUTPUT_PATH, index=False)\n",
        "print(f\"\\nâœ… Results saved to: {RESULTS_OUTPUT_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGWrgRL8WdPg",
        "outputId": "21877f3d-7289-47d2-d6b2-a3169d8bfa65"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph...\n",
            "Loaded 447 test questions.\n",
            "QREL file not found. Generating...\n",
            "âœ… QREL saved to: /content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/qrels.trec\n",
            "Loaded QRELs for 447 queries.\n",
            "Preparing BM25 index...\n",
            "BM25 ready.\n",
            "\n",
            "=== BM25 Retrieval ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BM25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [00:46<00:00,  9.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dense Evaluation: mpnet ===\n",
            "â†’ Context: passage_only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mpnet/passage_only: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [00:54<00:00,  8.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â†’ Context: parent\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mpnet/parent: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [00:53<00:00,  8.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â†’ Context: parent_child\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mpnet/parent_child: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [00:53<00:00,  8.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â†’ Context: full_neighborhood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mpnet/full_neighborhood: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [01:19<00:00,  5.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dense Evaluation: e5-large ===\n",
            "â†’ Context: passage_only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\re5-large/passage_only:   0%|          | 0/447 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "e5-large/passage_only: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [00:59<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â†’ Context: parent\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "e5-large/parent: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [01:01<00:00,  7.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â†’ Context: parent_child\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "e5-large/parent_child: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [01:00<00:00,  7.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â†’ Context: full_neighborhood\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "e5-large/full_neighborhood: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 447/447 [00:58<00:00,  7.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Final Evaluation Results:\n",
            "   Model           Context       Method  Recall@10  MAP@10  nDCG@10\n",
            "    BM25               N/A Lexical Only     0.3474  0.2611   0.3409\n",
            "e5-large full_neighborhood        Dense     0.3759  0.2586   0.3371\n",
            "e5-large full_neighborhood       Hybrid     0.4163  0.3052   0.3933\n",
            "e5-large            parent        Dense     0.3852  0.2710   0.3499\n",
            "e5-large            parent       Hybrid     0.4118  0.3043   0.3919\n",
            "e5-large      parent_child        Dense     0.3703  0.2566   0.3343\n",
            "e5-large      parent_child       Hybrid     0.4155  0.3069   0.3950\n",
            "e5-large      passage_only        Dense     0.3911  0.2818   0.3613\n",
            "e5-large      passage_only       Hybrid     0.4148  0.3053   0.3931\n",
            "   mpnet full_neighborhood        Dense     0.3404  0.2311   0.3038\n",
            "   mpnet full_neighborhood       Hybrid     0.4051  0.2872   0.3744\n",
            "   mpnet            parent        Dense     0.3474  0.2454   0.3177\n",
            "   mpnet            parent       Hybrid     0.4033  0.2909   0.3776\n",
            "   mpnet      parent_child        Dense     0.3357  0.2254   0.2975\n",
            "   mpnet      parent_child       Hybrid     0.4040  0.2870   0.3741\n",
            "   mpnet      passage_only        Dense     0.3708  0.2552   0.3335\n",
            "   mpnet      passage_only       Hybrid     0.4094  0.2912   0.3792\n",
            "\n",
            "âœ… Results saved to: /content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/experiment_1_full_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "z-Zp2gDx6JSw",
        "outputId": "c17299f3-457a-4eaf-bca1-149b3025e5d2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"https://www.myinstants.com/media/sounds/money-soundfx.mp3\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from Experiment 1: Baseline Retriever Performance**\n",
        "The results from the initial baseline experiment provide several clear and important insights into the effectiveness of different retrieval strategies for your dataset.\n",
        "\n",
        "**Observation 1: Hybrid Retrieval is a Clear Winner**\n",
        "In every single test case, the Hybrid method (combining dense semantic search with lexical BM25) significantly outperformed both the Dense (semantic only) and the Lexical Only (BM25) methods. This holds true across all metrics, including nDCG@10, Recall, and MAP. This strongly indicates that for your specific regulatory documents, combining keyword relevance with semantic meaning is the most effective initial retrieval strategy.\n",
        "\n",
        "**Observation 2: e5-large is the Superior Embedding Model**\n",
        "Across all context strategies and retrieval methods, the e5-large model consistently produced better results than the mpnet model. This suggests that e5-large has a better grasp of the nuances of your legal and regulatory text, leading to better recall and overall ranking quality.\n",
        "\n",
        "**Observation 3: Graph Context is Beneficial (with a Trade-off)**\n",
        "When evaluating the top-performing e5-large model across all three key metrics, a clear picture emerges:\n",
        "\n",
        "* Best Overall Ranking Quality (nDCG@10 & MAP@10): The parent_child context achieved the highest scores for both nDCG@10 (0.3950) and MAP@10 (0.3069). Since both of these metrics heavily reward placing correct documents at the very top of the ranked list, this configuration is the most precise.\n",
        "\n",
        "* Best for Finding Documents (Recall@10): The full_neighborhood context achieved the highest recall (0.4163). This indicates it was the most effective at finding all relevant passages and placing them somewhere within the top 10 results, even if not perfectly ordered.\n",
        "\n",
        "**Conclusion:** Identifying the Champion and Runner-Up\n",
        "Based on a balanced view of all three metrics, we can identify a clear champion for the next stage of experiments.\n",
        "\n",
        "* Champion Retriever: e5-large model with parent_child context, using the Hybrid method. It is the winner on two of the three most important metrics (nDCG@10 and MAP@10), making it the best-performing retriever for delivering highly relevant results at the top of the list.\n",
        "\n",
        "* Strong Runner-Up: e5-large model with full_neighborhood context, using the Hybrid method. While its ranking precision is a fraction lower, it excels at recall, making it an excellent and very close alternative.\n",
        "\n",
        "Since the performance of these two is extremely close, they both represent excellent starting points for our more advanced re-ranking experiments."
      ],
      "metadata": {
        "id": "fAm4TyMngOnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update After Experiment 2:** The Limits of Heuristic Graph Re-ranking\n",
        "The results from the enriched graph re-ranking experiment are in, and they provide a crucial insight: the graph-based re-ranking strategies did not improve performance.\n",
        "\n",
        "Experiment 1 Champion (Hybrid Retriever): nDCG@10 of 0.3950\n",
        "\n",
        "Experiment 2 Best (Graph Re-ranked): nDCG@10 of 0.3400\n",
        "\n",
        "This performance drop is a valuable finding. It suggests that the initial hybrid retrieval stage is already very effective. The simple, additive graph bonuses (parent, citation, etc.) were not nuanced enough to improve upon this strong baseline and, in some cases, likely introduced noise that harmed the ranking.\n",
        "\n",
        "This is not a failure, but a confirmation that to achieve the next level of precision, a more powerful re-ranking method is required. This leads us directly to the next logical step in our plan."
      ],
      "metadata": {
        "id": "6Icy2rh-gRvQ"
      }
    }
  ]
}