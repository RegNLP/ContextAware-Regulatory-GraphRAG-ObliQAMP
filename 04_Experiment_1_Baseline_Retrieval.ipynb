{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1koISL7wv9WHqi4b6D6rMIqlw8SHLb0Nh",
      "authorship_tag": "ABX9TyMRFtDpuUWAqhoKE9UcLSiQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/04_Experiment_1_Baseline_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rwC4LrN56B2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section evaluates multiple retriever models‚Äîpretrained, fine-tuned, and advanced fine-tuned‚Äîusing both lexical and dense retrieval strategies over a graph-structured regulatory dataset. The goal is to identify the most effective retriever configuration for multi-passage question answering.\n",
        "\n",
        "**‚úÖ Evaluation Steps**\n",
        "* 1Ô∏è‚É£ Setup and Configuration\n",
        "  * Install essential libraries: sentence-transformers, rank_bm25, pytrec_eval, etc.\n",
        "  * Define paths for:\n",
        "    * Document graph (graph.gpickle)\n",
        "    * Test dataset (ObliQA_MultiPassage_test.json)\n",
        "    * Precomputed embeddings\n",
        "    * Output folders for JSON and CSV results\n",
        "\n",
        "* 2Ô∏è‚É£ Load Data\n",
        "  * Load the regulatory graph structure with passages as nodes.\n",
        "  * Load the test question set (JSON), where each question is linked to relevant passages.\n",
        "  * Generate or load QRELs in TREC format (mapping of question IDs to relevant passage IDs).\n",
        "\n",
        "* 3Ô∏è‚É£ Prepare BM25 Retriever\n",
        "  * Extract passage texts from graph nodes.\n",
        "  * Tokenize each passage into words.\n",
        "  * Use the BM25Okapi class to initialize a lexical BM25 retriever over the passage corpus.\n",
        "\n",
        "* 4Ô∏è‚É£ Define Evaluation Helpers\n",
        "  * add_instruction_to_query: Adds model-specific query prompts (e.g., \"query: \" for E5).\n",
        "  * evaluate_run: Evaluates a retrieval run using Recall@10 and MAP@10 with pytrec_eval.\n",
        "  * reciprocal_rank_fusion: Combines multiple rankings into one (used for hybrid retrieval).\n",
        "  * format_run_for_json: Converts internal retrieval results into readable JSON format, including passage text, scores, and internal IDs.\n",
        "\n",
        "* 5Ô∏è‚É£ Run BM25 Retrieval\n",
        "\n",
        "  * For each question:\n",
        "    * Use BM25 to score all passages.\n",
        "    * Select the top-ùëò (e.g., 100) passages by score.\n",
        "    * Evaluate BM25 results using pytrec_eval.\n",
        "\n",
        "  * Save:\n",
        "    * Summary metrics (Recall@10, MAP@10)\n",
        "    * Detailed retrieved passages (as JSON)\n",
        "\n",
        "* 6Ô∏è‚É£ Dense and Hybrid Retrieval (Per Model and Context)\n",
        "\n",
        "  * For each retriever model (e.g., e5-large-v2, all-mpnet-base-v2, bge-base-en-v1.5) and each graph context variant (e.g., passage_only, parent, parent_child, parent_child_cites, full_neighborhood), we evaluate both Dense and Hybrid retrieval strategies:\n",
        "\n",
        "    * a. Load Embeddings\n",
        "        \n",
        "      * Load passage embeddings (embeddings.pkl) and corresponding passage IDs (passage_ids.json) for the current model-context configuration.\n",
        "      \n",
        "      * If missing, skip the current configuration.\n",
        "\n",
        "    * b. Encode Queries\n",
        "      \n",
        "      * Add an instruction prefix to the query, if required by the model (e.g., E5 or BGE).\n",
        "\n",
        "      * Encode the query into a dense embedding using the selected SentenceTransformer.\n",
        "\n",
        "    * c. Dense Retrieval\n",
        "      \n",
        "      * Compute cosine similarity between the query embedding and all passage embeddings.\n",
        "\n",
        "      * Select the top-ùëò most similar passages.\n",
        "\n",
        "      * Format and store the run (passage IDs and similarity scores).\n",
        "\n",
        "    * d. Hybrid Retrieval (Reciprocal Rank Fusion)\n",
        "      \n",
        "      * Get top-ùëò BM25 matches for the same query.\n",
        "\n",
        "      * Combine the BM25 and Dense results using Reciprocal Rank Fusion (RRF):\n",
        "\n",
        "          RRF¬†Score=‚àëùëü‚ààretrievers1ùëò+rankùëüRRF¬†Score= r‚ààretrievers‚àëk+rank r1‚Äã\n",
        "\n",
        "      * Select the top-ùëò fused results as the Hybrid retrieval output.\n",
        "\n",
        "    * e. Evaluate and Save\n",
        "\n",
        "      * Evaluate both Dense and Hybrid results using:\n",
        "\n",
        "        * Recall@10: Percentage of queries for which at least one relevant passage was retrieved in the top 10.\n",
        "\n",
        "        * MAP@10: Average precision of relevant documents among the top 10.\n",
        "\n",
        "      * Save:\n",
        "\n",
        "        * Metrics to summary list\n",
        "\n",
        "        * Retrieved passage details (texts, scores, passage IDs) to JSON files\n",
        "\n",
        "* 7Ô∏è‚É£ Aggregate and Report Final Results\n",
        "\n",
        "  After evaluating all model-context-method combinations:\n",
        "\n",
        "    * a. Combine All Metrics\n",
        "\n",
        "      * Collect all results (BM25, Dense, Hybrid) into a pandas DataFrame.\n",
        "\n",
        "      * Each row represents a unique configuration:\n",
        "\n",
        "        Retriever model\n",
        "\n",
        "        Graph context setting\n",
        "\n",
        "        Retrieval method\n",
        "\n",
        "        Recall@10 and MAP@10 scores\n",
        "\n",
        "    * b. Sort and Rank\n",
        "      * Sort the DataFrame in descending order of Recall@10 and MAP@10 to rank retrievers.\n",
        "\n",
        "      * The top row represents the best-performing configuration across all methods.\n",
        "\n",
        "    * c. Save Final Outputs\n",
        "      * Save the summary DataFrame as a CSV file (experiment_1_final_retriever_comparison_results.csv)\n",
        "\n",
        "      * Print the complete table in the notebook for inspection.\n",
        "\n",
        "      * Highlight and print the best-performing configuration.\n",
        "\n",
        "      * All per-query results (passage texts, scores) are also saved as JSON files in the output folder."
      ],
      "metadata": {
        "id": "w88X9WwMPSId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Final Retriever Evaluation: Pre-trained vs. Fine-Tuned vs. Advanced\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Evaluate all retriever models: pre-trained, standard fine-tuned, and\n",
        "#    advanced fine-tuned (with hard negatives).\n",
        "# 2. Apply instruction prefixes during inference for relevant models.\n",
        "# 3. Compare BM25, Dense, and Hybrid retrieval methods to identify the\n",
        "#    definitive champion retriever.\n",
        "# 4. Save both a CSV summary and detailed JSON outputs for each run.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25 pytrec_eval\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings_full_comparison\")\n",
        "\n",
        "# --- Output Folders ---\n",
        "RESULTS_CSV_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_1_final_retriever_comparison_results.csv\")\n",
        "RESULTS_JSON_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"experiment_1_retrieval_results_json\")\n",
        "os.makedirs(RESULTS_JSON_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Model Input Folders ---\n",
        "FINETUNED_RETRIEVER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "ADVANCED_FINETUNED_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers_advanced\")\n",
        "\n",
        "K = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Models and Contexts ---\n",
        "MODELS_TO_EVALUATE = {\n",
        "    \"e5-large-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"bge-base-en-v1.5\"),\n",
        "    \"e5-large-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"bge-base-en-v1.5\"),\n",
        "    \"e5-large-v2_Pretrained\": \"intfloat/e5-large-v2\",\n",
        "    \"all-mpnet-base-v2_Pretrained\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"bge-base-en-v1.5_Pretrained\": \"BAAI/bge-base-en-v1.5\",\n",
        "}\n",
        "CONTEXT_CONFIGS = [\"passage_only\", \"parent\", \"parent_child\", \"parent_child_cites\", \"full_neighborhood\"]\n",
        "\n",
        "# --- Load Graph and Data ---\n",
        "print(\"Loading graph and test data...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "# Create a quick lookup for question text by QID\n",
        "qid_to_question = {q[\"QuestionID\"]: q[\"Question\"] for q in test_data}\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# --- Generate and Load QRELs ---\n",
        "if not os.path.exists(QREL_PATH):\n",
        "    print(\"QREL file not found. Generating...\")\n",
        "    with open(QREL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            qid = item[\"QuestionID\"]\n",
        "            for passage in item[\"Passages\"]:\n",
        "                uid = f\"{passage['DocumentID']}|||{passage['PassageID']}\"\n",
        "                f.write(f\"{qid} 0 {uid} 1\\n\")\n",
        "    print(f\"‚úÖ QREL saved to: {QREL_PATH}\")\n",
        "else:\n",
        "    print(\"QREL file found.\")\n",
        "\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# --- Prepare BM25 ---\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "# Create a map from combined UID to internal UID for text lookup\n",
        "uid_map = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": uid for uid in all_passage_uids}\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def add_instruction_to_query(query, model_key):\n",
        "    if \"e5\" in model_key:\n",
        "        return f\"query: {query}\"\n",
        "    if \"bge\" in model_key:\n",
        "        return f\"Represent this sentence for searching relevant passages: {query}\"\n",
        "    return query\n",
        "\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def format_run_for_json(run_dict, qid_to_question_map, uid_to_internal_uid_map, graph, top_n=10):\n",
        "    output_list = []\n",
        "    for qid, passages in run_dict.items():\n",
        "        sorted_passages = sorted(passages.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        retrieved_passages_text = []\n",
        "        retrieved_scores = []\n",
        "        retrieved_ids = []\n",
        "\n",
        "        for combined_uid, score in sorted_passages[:top_n]:\n",
        "            internal_uid = uid_to_internal_uid_map.get(combined_uid)\n",
        "            if internal_uid:\n",
        "                retrieved_passages_text.append(graph.nodes[internal_uid].get(\"text\", \"\"))\n",
        "                retrieved_scores.append(score)\n",
        "                retrieved_ids.append(internal_uid)\n",
        "\n",
        "        output_list.append({\n",
        "            \"QuestionID\": qid,\n",
        "            \"Question\": qid_to_question_map.get(qid, \"\"),\n",
        "            \"RetrievedPassages\": retrieved_passages_text,\n",
        "            \"RetrievedScores\": retrieved_scores,\n",
        "            \"RetrievedIDs\": retrieved_ids\n",
        "        })\n",
        "    return output_list\n",
        "\n",
        "# --- Main Evaluation ---\n",
        "all_results = []\n",
        "\n",
        "# 1. BM25 Baseline\n",
        "print(\"\\n=== BM25 Retrieval ===\")\n",
        "bm25_run = {}\n",
        "for q in tqdm(test_data, desc=\"BM25\"):\n",
        "    qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    top_idxs = np.argsort(scores)[::-1][:K]\n",
        "    bm25_run[qid] = {f\"{G.nodes[all_passage_uids[idx]].get('document_id')}|||{G.nodes[all_passage_uids[idx]].get('passage_id')}\": float(scores[idx]) for idx in top_idxs}\n",
        "bm25_metrics = evaluate_run(bm25_run, qrel)\n",
        "all_results.append({\"Model\": \"BM25\", \"Context\": \"N/A\", \"Method\": \"Lexical Only\", **bm25_metrics})\n",
        "bm25_json = format_run_for_json(bm25_run, qid_to_question, uid_map, G)\n",
        "with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, \"BM25_results.json\"), \"w\") as f:\n",
        "    json.dump(bm25_json, f, indent=4)\n",
        "\n",
        "# 2. Dense and Hybrid Models\n",
        "for model_key, model_path in MODELS_TO_EVALUATE.items():\n",
        "    print(f\"\\n=== Evaluating Model: {model_key} ===\")\n",
        "    query_encoder = SentenceTransformer(model_path, device=device)\n",
        "    for context_key in CONTEXT_CONFIGS:\n",
        "        print(f\"‚Üí Context: {context_key}\")\n",
        "        emb_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"embeddings.pkl\")\n",
        "        id_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"passage_ids.json\")\n",
        "\n",
        "        try:\n",
        "            with open(emb_path, \"rb\") as f: passage_embeddings = pickle.load(f)\n",
        "            with open(id_path, \"r\") as f: passage_ids = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ö†Ô∏è Embeddings not found for {model_key}/{context_key}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        dense_run, hybrid_run = {}, {}\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "\n",
        "        for q in tqdm(test_data, desc=f\"{model_key}/{context_key}\"):\n",
        "            qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "\n",
        "            instructed_query = add_instruction_to_query(query, model_key)\n",
        "            query_emb = query_encoder.encode(instructed_query, convert_to_tensor=True, device=device)\n",
        "\n",
        "            cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "            top_results = torch.topk(cos_scores, k=min(K, len(passage_ids)))\n",
        "\n",
        "            dense_run[qid], dense_uids = {}, []\n",
        "            for idx, score in zip(top_results.indices, top_results.values):\n",
        "                uid = passage_ids[idx]\n",
        "                node = G.nodes[uid]\n",
        "                combined_uid = f\"{node.get('document_id')}|||{node.get('passage_id')}\"\n",
        "                dense_run[qid][combined_uid] = float(score.item())\n",
        "                dense_uids.append(uid)\n",
        "\n",
        "            bm25_scores = bm25.get_scores(query.split())\n",
        "            top_bm25 = np.argsort(bm25_scores)[::-1][:K]\n",
        "            bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "\n",
        "            fused_uids = reciprocal_rank_fusion([dense_uids, bm25_uids])[:K]\n",
        "            hybrid_run[qid] = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": score for uid, score in fused_uids}\n",
        "\n",
        "        dense_metrics = evaluate_run(dense_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Dense\", **dense_metrics})\n",
        "        dense_json = format_run_for_json(dense_run, qid_to_question, uid_map, G)\n",
        "        with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{model_key}_{context_key}_Dense_results.json\"), \"w\") as f:\n",
        "            json.dump(dense_json, f, indent=4)\n",
        "\n",
        "        hybrid_metrics = evaluate_run(hybrid_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Hybrid\", **hybrid_metrics})\n",
        "        hybrid_json = format_run_for_json(hybrid_run, qid_to_question, uid_map, G)\n",
        "        with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{model_key}_{context_key}_Hybrid_results.json\"), \"w\") as f:\n",
        "            json.dump(hybrid_json, f, indent=4)\n",
        "\n",
        "# --- Save and Display Final Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.rename(columns={\"recall_10\": \"Recall@10\", \"map_cut_10\": \"MAP@10\"})\n",
        "df = df.sort_values(by=[\"Recall@10\", \"MAP@10\"], ascending=False)\n",
        "\n",
        "print(\"\\nüìä Final Evaluation Results:\")\n",
        "print(df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "df.to_csv(RESULTS_CSV_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n‚úÖ CSV summary saved to: {RESULTS_CSV_OUTPUT_PATH}\")\n",
        "print(f\"‚úÖ Detailed JSON results saved to: {RESULTS_JSON_OUTPUT_FOLDER}\")\n",
        "\n",
        "print(\"\\n--- üèÜ Best Performing Configuration ---\")\n",
        "print(df.iloc[0])\n"
      ],
      "metadata": {
        "id": "JGWrgRL8WdPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Final Evaluation Results:\n",
        "                        Model            Context       Method  MAP@10  Recall@10\n",
        "               e5-large-v2_FT parent_child_cites       Hybrid  0.3319     0.4497\n",
        "               e5-large-v2_FT  full_neighborhood       Hybrid  0.3318     0.4497\n",
        "               e5-large-v2_FT       parent_child       Hybrid  0.3316     0.4497\n",
        "               e5-large-v2_FT       passage_only       Hybrid  0.3288     0.4465\n",
        "               e5-large-v2_FT             parent       Hybrid  0.3304     0.4459\n",
        "               e5-large-v2_FT       passage_only        Dense  0.3158     0.4445\n",
        "               e5-large-v2_FT             parent        Dense  0.3064     0.4433\n",
        "         all-mpnet-base-v2_FT parent_child_cites       Hybrid  0.3208     0.4403\n",
        "         all-mpnet-base-v2_FT  full_neighborhood       Hybrid  0.3205     0.4392\n",
        "         all-mpnet-base-v2_FT       parent_child       Hybrid  0.3195     0.4385\n",
        "         all-mpnet-base-v2_FT             parent       Hybrid  0.3207     0.4381\n",
        "               e5-large-v2_FT       parent_child        Dense  0.3003     0.4345\n",
        "          bge-base-en-v1.5_FT parent_child_cites       Hybrid  0.3180     0.4324\n",
        "          bge-base-en-v1.5_FT  full_neighborhood       Hybrid  0.3179     0.4324\n",
        "               e5-large-v2_FT  full_neighborhood        Dense  0.2996     0.4323\n",
        "               e5-large-v2_FT parent_child_cites        Dense  0.2990     0.4323\n",
        "         all-mpnet-base-v2_FT       passage_only       Hybrid  0.3197     0.4323\n",
        "          bge-base-en-v1.5_FT       parent_child       Hybrid  0.3181     0.4312\n",
        "          bge-base-en-v1.5_FT             parent       Hybrid  0.3188     0.4286\n",
        "          bge-base-en-v1.5_FT       passage_only       Hybrid  0.3152     0.4227\n",
        "          bge-base-en-v1.5_FT             parent        Dense  0.2857     0.4207\n",
        "          bge-base-en-v1.5_FT       passage_only        Dense  0.2919     0.4196\n",
        "       e5-large-v2_Pretrained       passage_only       Hybrid  0.3087     0.4176\n",
        "      e5-large-v2_FT_Advanced parent_child_cites       Hybrid  0.3097     0.4174\n",
        "       e5-large-v2_Pretrained parent_child_cites       Hybrid  0.3097     0.4174\n",
        "       e5-large-v2_Pretrained  full_neighborhood       Hybrid  0.3088     0.4174\n",
        "      e5-large-v2_FT_Advanced  full_neighborhood       Hybrid  0.3086     0.4174\n",
        "      e5-large-v2_FT_Advanced             parent       Hybrid  0.3085     0.4174\n",
        "       e5-large-v2_Pretrained             parent       Hybrid  0.3084     0.4174\n",
        "      e5-large-v2_FT_Advanced       parent_child       Hybrid  0.3084     0.4167\n",
        "       e5-large-v2_Pretrained       parent_child       Hybrid  0.3084     0.4167\n",
        "      e5-large-v2_FT_Advanced       passage_only       Hybrid  0.3091     0.4165\n",
        "  bge-base-en-v1.5_Pretrained parent_child_cites       Hybrid  0.2996     0.4153\n",
        " bge-base-en-v1.5_FT_Advanced parent_child_cites       Hybrid  0.2995     0.4142\n",
        "  bge-base-en-v1.5_Pretrained  full_neighborhood       Hybrid  0.2993     0.4138\n",
        " bge-base-en-v1.5_FT_Advanced  full_neighborhood       Hybrid  0.2991     0.4127\n",
        "  bge-base-en-v1.5_Pretrained       passage_only       Hybrid  0.2979     0.4116\n",
        " bge-base-en-v1.5_FT_Advanced       passage_only       Hybrid  0.2971     0.4116\n",
        "  bge-base-en-v1.5_Pretrained       parent_child       Hybrid  0.2998     0.4101\n",
        "all-mpnet-base-v2_FT_Advanced       passage_only       Hybrid  0.2905     0.4094\n",
        " all-mpnet-base-v2_Pretrained       passage_only       Hybrid  0.2905     0.4094\n",
        "         all-mpnet-base-v2_FT       passage_only        Dense  0.2877     0.4094\n",
        "          bge-base-en-v1.5_FT  full_neighborhood        Dense  0.2747     0.4091\n",
        " bge-base-en-v1.5_FT_Advanced       parent_child       Hybrid  0.2996     0.4090\n",
        "          bge-base-en-v1.5_FT parent_child_cites        Dense  0.2744     0.4080\n",
        "all-mpnet-base-v2_FT_Advanced             parent       Hybrid  0.2908     0.4066\n",
        "          bge-base-en-v1.5_FT       parent_child        Dense  0.2722     0.4065\n",
        "  bge-base-en-v1.5_Pretrained             parent       Hybrid  0.2989     0.4056\n",
        "all-mpnet-base-v2_FT_Advanced       parent_child       Hybrid  0.2874     0.4051\n",
        " all-mpnet-base-v2_Pretrained  full_neighborhood       Hybrid  0.2871     0.4051\n",
        "all-mpnet-base-v2_FT_Advanced  full_neighborhood       Hybrid  0.2869     0.4051\n",
        " bge-base-en-v1.5_FT_Advanced             parent       Hybrid  0.2985     0.4045\n",
        " all-mpnet-base-v2_Pretrained       parent_child       Hybrid  0.2870     0.4040\n",
        " all-mpnet-base-v2_Pretrained             parent       Hybrid  0.2903     0.4033\n",
        " all-mpnet-base-v2_Pretrained parent_child_cites       Hybrid  0.2869     0.4029\n",
        "all-mpnet-base-v2_FT_Advanced parent_child_cites       Hybrid  0.2867     0.4029\n",
        "         all-mpnet-base-v2_FT             parent        Dense  0.2817     0.4009\n",
        "         all-mpnet-base-v2_FT parent_child_cites        Dense  0.2825     0.3975\n",
        "         all-mpnet-base-v2_FT  full_neighborhood        Dense  0.2819     0.3949\n",
        "         all-mpnet-base-v2_FT       parent_child        Dense  0.2789     0.3938\n",
        "      e5-large-v2_FT_Advanced       passage_only        Dense  0.2890     0.3930\n",
        "       e5-large-v2_Pretrained       passage_only        Dense  0.2892     0.3928\n",
        "      e5-large-v2_FT_Advanced             parent        Dense  0.2826     0.3886\n",
        "       e5-large-v2_Pretrained             parent        Dense  0.2823     0.3886\n",
        "      e5-large-v2_FT_Advanced       parent_child        Dense  0.2740     0.3866\n",
        "       e5-large-v2_Pretrained       parent_child        Dense  0.2738     0.3866\n",
        "      e5-large-v2_FT_Advanced parent_child_cites        Dense  0.2749     0.3855\n",
        "       e5-large-v2_Pretrained parent_child_cites        Dense  0.2746     0.3855\n",
        "       e5-large-v2_Pretrained  full_neighborhood        Dense  0.2746     0.3855\n",
        "      e5-large-v2_FT_Advanced  full_neighborhood        Dense  0.2747     0.3844\n",
        "  bge-base-en-v1.5_Pretrained       passage_only        Dense  0.2751     0.3795\n",
        " bge-base-en-v1.5_FT_Advanced       passage_only        Dense  0.2748     0.3787\n",
        "  bge-base-en-v1.5_Pretrained             parent        Dense  0.2690     0.3742\n",
        " bge-base-en-v1.5_FT_Advanced             parent        Dense  0.2687     0.3735\n",
        " all-mpnet-base-v2_Pretrained       passage_only        Dense  0.2552     0.3708\n",
        "all-mpnet-base-v2_FT_Advanced       passage_only        Dense  0.2549     0.3708\n",
        "  bge-base-en-v1.5_Pretrained  full_neighborhood        Dense  0.2542     0.3668\n",
        " bge-base-en-v1.5_FT_Advanced  full_neighborhood        Dense  0.2539     0.3660\n",
        "  bge-base-en-v1.5_Pretrained       parent_child        Dense  0.2516     0.3657\n",
        " bge-base-en-v1.5_FT_Advanced       parent_child        Dense  0.2514     0.3649\n",
        " bge-base-en-v1.5_FT_Advanced parent_child_cites        Dense  0.2541     0.3634\n",
        "  bge-base-en-v1.5_Pretrained parent_child_cites        Dense  0.2541     0.3630\n",
        "all-mpnet-base-v2_FT_Advanced             parent        Dense  0.2456     0.3485\n",
        "                         BM25                N/A Lexical Only  0.2611     0.3474\n",
        " all-mpnet-base-v2_Pretrained             parent        Dense  0.2454     0.3474\n",
        " all-mpnet-base-v2_Pretrained parent_child_cites        Dense  0.2271     0.3387\n",
        "all-mpnet-base-v2_FT_Advanced parent_child_cites        Dense  0.2268     0.3387\n",
        " all-mpnet-base-v2_Pretrained  full_neighborhood        Dense  0.2269     0.3383\n",
        "all-mpnet-base-v2_FT_Advanced  full_neighborhood        Dense  0.2267     0.3383\n",
        "all-mpnet-base-v2_FT_Advanced       parent_child        Dense  0.2255     0.3357\n",
        " all-mpnet-base-v2_Pretrained       parent_child        Dense  0.2254     0.3357\n",
        "\n",
        "‚úÖ CSV summary saved to: /content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/experiment_1_final_retriever_comparison_results.csv\n",
        "‚úÖ Detailed JSON results saved to: /content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/experiment_1_retrieval_results_json\n",
        "\n",
        "--- üèÜ Best Performing Configuration ---\n",
        "Model            e5-large-v2_FT\n",
        "Context      parent_child_cites\n",
        "Method                   Hybrid\n",
        "MAP@10                 0.331936\n",
        "Recall@10              0.449664\n",
        "Name: 38, dtype: object"
      ],
      "metadata": {
        "id": "QqD8SR6IXLVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "z-Zp2gDx6JSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Observations from the Final Retriever Evaluation\n",
        "Standard Fine-Tuning (_FT) Provides a Huge Boost: This is the most significant finding. The standard fine-tuned models (e5-large-v2_FT, all-mpnet-base-v2_FT, etc.) dramatically outperform their _Pretrained counterparts.\n",
        "\n",
        "e5-large-v2_FT (Recall@10: 0.4497) is a massive improvement over e5-large-v2_Pretrained (Recall@10: 0.4176).\n",
        "\n",
        "This proves that adapting the retriever to your specific regulatory domain is a highly effective strategy.\n",
        "\n",
        "Advanced Fine-Tuning (_FT_Advanced) Did Not Help: This is a very interesting and important \"negative result\" for your paper. The advanced fine-tuning with hard negatives and instruction tuning did not improve performance over the standard fine-tuning. In fact, the e5-large-v2_FT_Advanced model's performance is almost identical to the e5-large-v2_Pretrained model. This suggests that for your dataset, the simpler fine-tuning approach was more effective.\n",
        "\n",
        "Hybrid is Still King: For all the top-performing models, the Hybrid method is consistently better than the Dense-only method, confirming our earlier finding.\n",
        "\n",
        "Context Matters, but Less Than Fine-Tuning: The various context strategies (parent_child_cites, full_neighborhood, etc.) all perform very similarly for the best models. The most important factor by far was the standard fine-tuning.\n",
        "\n",
        "Conclusion: We Have a New Champion\n",
        "The results are definitive. The best-performing retriever is:\n",
        "\n",
        "üèÜ New Champion Retriever: The e5-large-v2_FT (standard fine-tuned) model, using the parent_child_cites context and the Hybrid method.\n",
        "\n",
        "This configuration achieved the highest scores across both Recall@10 (0.4497) and MAP@10 (0.3319).\n",
        "\n"
      ],
      "metadata": {
        "id": "3cP07_-FXeVo"
      }
    }
  ]
}