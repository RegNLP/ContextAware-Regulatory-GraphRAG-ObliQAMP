{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1koISL7wv9WHqi4b6D6rMIqlw8SHLb0Nh",
      "authorship_tag": "ABX9TyPncZ7cuBmVEO9I/5mP4KKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/04_Experiment_1_Baseline_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rwC4LrN56B2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Re-evaluation of Baseline Retrievers (with Fine-Tuned Models)\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Evaluate the performance of pre-trained and fine-tuned dense\n",
        "#    retriever models.\n",
        "# 2. Automatically generate passage embeddings for any model/context\n",
        "#    combination if they don't already exist.\n",
        "# 3. Compare BM25, Dense, and Hybrid retrieval methods to identify the\n",
        "#    definitive champion retriever.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25 pytrec_eval\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings_finetuned\") # Use a new folder for clarity\n",
        "RESULTS_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_1_rerun_with_finetuned_results.csv\")\n",
        "FINETUNED_RETRIEVER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "\n",
        "os.makedirs(EMBEDDINGS_FOLDER, exist_ok=True)\n",
        "\n",
        "K = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Embedding Models and Contexts ---\n",
        "# Combine pre-trained and fine-tuned models for a full comparison\n",
        "MODELS_TO_EVALUATE = {\n",
        "    \"e5-large-v2_Pretrained\": \"intfloat/e5-large-v2\",\n",
        "    \"all-mpnet-base-v2_Pretrained\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"bge-base-en-v1.5_Pretrained\": \"BAAI/bge-base-en-v1.5\",\n",
        "    \"e5-large-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"bge-base-en-v1.5\"),\n",
        "}\n",
        "CONTEXT_CONFIGS = [\"passage_only\", \"parent\", \"parent_child\", \"full_neighborhood\"]\n",
        "\n",
        "# --- Load Graph and Data ---\n",
        "print(\"Loading graph and test data...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# --- Generate and Load QRELs ---\n",
        "if not os.path.exists(QREL_PATH):\n",
        "    print(\"QREL file not found. Generating...\")\n",
        "    with open(QREL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            qid = item[\"QuestionID\"]\n",
        "            for passage in item[\"Passages\"]:\n",
        "                uid = f\"{passage['DocumentID']}|||{passage['PassageID']}\"\n",
        "                f.write(f\"{qid} 0 {uid} 1\\n\")\n",
        "    print(f\"✅ QREL saved to: {QREL_PATH}\")\n",
        "else:\n",
        "    print(\"QREL file found.\")\n",
        "\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# --- Prepare BM25 ---\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def build_contextual_text(G, node_id, get_neighbors_func):\n",
        "    base_text = G.nodes[node_id].get(\"text\", \"\")\n",
        "    context_parts = [base_text]\n",
        "    neighbors = get_neighbors_func(G, node_id)\n",
        "    for neighbor_id in neighbors:\n",
        "        context_text = G.nodes[neighbor_id].get(\"text\", \"\")\n",
        "        if context_text:\n",
        "            context_parts.append(context_text)\n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "neighbor_logic = {\n",
        "    \"passage_only\": lambda G, node: [],\n",
        "    \"parent\": lambda G, node: list(G.predecessors(node)),\n",
        "    \"parent_child\": lambda G, node: list(G.predecessors(node)) + list(G.successors(node)),\n",
        "    \"full_neighborhood\": lambda G, node: list(nx.neighbors(G, node))\n",
        "}\n",
        "\n",
        "# --- Auto-Generate Embeddings if Missing ---\n",
        "print(\"\\nChecking for and generating missing embeddings...\")\n",
        "for model_key, model_path in MODELS_TO_EVALUATE.items():\n",
        "    for context_key in CONTEXT_CONFIGS:\n",
        "        out_dir = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key)\n",
        "        emb_path = os.path.join(out_dir, \"embeddings.pkl\")\n",
        "        if not os.path.exists(emb_path):\n",
        "            print(f\"⚠️ Embeddings not found for {model_key}/{context_key}. Generating now...\")\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "            model = SentenceTransformer(model_path, device=device)\n",
        "\n",
        "            texts_to_encode, uids_to_save = [], []\n",
        "            for node_id in tqdm(all_passage_uids, desc=f\"Building context for {model_key}/{context_key}\"):\n",
        "                full_text = build_contextual_text(G, node_id, neighbor_logic[context_key])\n",
        "                texts_to_encode.append(full_text)\n",
        "                uids_to_save.append(node_id)\n",
        "\n",
        "            embeddings = model.encode(texts_to_encode, show_progress_bar=True, batch_size=32)\n",
        "\n",
        "            with open(os.path.join(out_dir, \"passage_ids.json\"), \"w\") as f:\n",
        "                json.dump(uids_to_save, f)\n",
        "            with open(emb_path, \"wb\") as f:\n",
        "                pickle.dump(embeddings, f)\n",
        "            print(f\"✅ Saved embeddings for {model_key}/{context_key}\")\n",
        "        else:\n",
        "            print(f\"✅ Embeddings found for {model_key}/{context_key}.\")\n",
        "\n",
        "# --- Evaluation and Fusion Functions ---\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\", \"ndcg_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# --- Main Evaluation ---\n",
        "all_results = []\n",
        "\n",
        "# 1. BM25 Baseline\n",
        "print(\"\\n=== BM25 Retrieval ===\")\n",
        "bm25_run = {}\n",
        "for q in tqdm(test_data, desc=\"BM25\"):\n",
        "    qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    top_idxs = np.argsort(scores)[::-1][:K]\n",
        "    bm25_run[qid] = {f\"{G.nodes[all_passage_uids[idx]].get('document_id')}|||{G.nodes[all_passage_uids[idx]].get('passage_id')}\": float(scores[idx]) for idx in top_idxs}\n",
        "bm25_metrics = evaluate_run(bm25_run, qrel)\n",
        "all_results.append({\"Model\": \"BM25\", \"Context\": \"N/A\", \"Method\": \"Lexical Only\", **bm25_metrics})\n",
        "\n",
        "# 2. Dense and Hybrid Models\n",
        "for model_key, model_path in MODELS_TO_EVALUATE.items():\n",
        "    print(f\"\\n=== Evaluating Model: {model_key} ===\")\n",
        "    query_encoder = SentenceTransformer(model_path, device=device)\n",
        "    for context_key in CONTEXT_CONFIGS:\n",
        "        print(f\"→ Context: {context_key}\")\n",
        "        emb_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"embeddings.pkl\")\n",
        "        id_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"passage_ids.json\")\n",
        "\n",
        "        with open(emb_path, \"rb\") as f: passage_embeddings = pickle.load(f)\n",
        "        with open(id_path, \"r\") as f: passage_ids = json.load(f)\n",
        "\n",
        "        dense_run, hybrid_run = {}, {}\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "\n",
        "        for q in tqdm(test_data, desc=f\"{model_key}/{context_key}\"):\n",
        "            qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "            query_emb = query_encoder.encode(query, convert_to_tensor=True, device=device)\n",
        "            cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "            top_results = torch.topk(cos_scores, k=min(K, len(passage_ids)))\n",
        "\n",
        "            dense_run[qid], dense_uids = {}, []\n",
        "            for idx, score in zip(top_results.indices, top_results.values):\n",
        "                uid = passage_ids[idx]\n",
        "                node = G.nodes[uid]\n",
        "                combined_uid = f\"{node.get('document_id')}|||{node.get('passage_id')}\"\n",
        "                dense_run[qid][combined_uid] = float(score.item())\n",
        "                dense_uids.append(uid)\n",
        "\n",
        "            bm25_scores = bm25.get_scores(query.split())\n",
        "            top_bm25 = np.argsort(bm25_scores)[::-1][:K]\n",
        "            bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "\n",
        "            fused_uids = reciprocal_rank_fusion([dense_uids, bm25_uids])[:K]\n",
        "            hybrid_run[qid] = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": score for uid, score in fused_uids}\n",
        "\n",
        "        dense_metrics = evaluate_run(dense_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Dense\", **dense_metrics})\n",
        "        hybrid_metrics = evaluate_run(hybrid_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Hybrid\", **hybrid_metrics})\n",
        "\n",
        "# --- Save and Display Final Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "# Rename columns for clarity in the final table\n",
        "df = df.rename(columns={\"recall_10\": \"Recall@10\", \"map_cut_10\": \"MAP@10\", \"ndcg_cut_10\": \"nDCG@10\"})\n",
        "df = df.sort_values(by=\"nDCG@10\", ascending=False)\n",
        "\n",
        "print(\"\\n📊 Final Evaluation Results:\")\n",
        "print(df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "df.to_csv(RESULTS_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n✅ Results saved to: {RESULTS_OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\n--- 🏆 Best Performing Configuration ---\")\n",
        "print(df.iloc[0])\n"
      ],
      "metadata": {
        "id": "JGWrgRL8WdPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "z-Zp2gDx6JSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from Experiment 1: Baseline Retriever Performance**\n",
        "The results from the initial baseline experiment provide several clear and important insights into the effectiveness of different retrieval strategies for your dataset.\n",
        "\n",
        "**Observation 1: Hybrid Retrieval is a Clear Winner**\n",
        "In every single test case, the Hybrid method (combining dense semantic search with lexical BM25) significantly outperformed both the Dense (semantic only) and the Lexical Only (BM25) methods. This holds true across all metrics, including nDCG@10, Recall, and MAP. This strongly indicates that for your specific regulatory documents, combining keyword relevance with semantic meaning is the most effective initial retrieval strategy.\n",
        "\n",
        "**Observation 2: e5-large is the Superior Embedding Model**\n",
        "Across all context strategies and retrieval methods, the e5-large model consistently produced better results than the mpnet model. This suggests that e5-large has a better grasp of the nuances of your legal and regulatory text, leading to better recall and overall ranking quality.\n",
        "\n",
        "**Observation 3: Graph Context is Beneficial (with a Trade-off)**\n",
        "When evaluating the top-performing e5-large model across all three key metrics, a clear picture emerges:\n",
        "\n",
        "* Best Overall Ranking Quality (nDCG@10 & MAP@10): The parent_child context achieved the highest scores for both nDCG@10 (0.3950) and MAP@10 (0.3069). Since both of these metrics heavily reward placing correct documents at the very top of the ranked list, this configuration is the most precise.\n",
        "\n",
        "* Best for Finding Documents (Recall@10): The full_neighborhood context achieved the highest recall (0.4163). This indicates it was the most effective at finding all relevant passages and placing them somewhere within the top 10 results, even if not perfectly ordered.\n",
        "\n",
        "**Conclusion:** Identifying the Champion and Runner-Up\n",
        "Based on a balanced view of all three metrics, we can identify a clear champion for the next stage of experiments.\n",
        "\n",
        "* Champion Retriever: e5-large model with parent_child context, using the Hybrid method. It is the winner on two of the three most important metrics (nDCG@10 and MAP@10), making it the best-performing retriever for delivering highly relevant results at the top of the list.\n",
        "\n",
        "* Strong Runner-Up: e5-large model with full_neighborhood context, using the Hybrid method. While its ranking precision is a fraction lower, it excels at recall, making it an excellent and very close alternative.\n",
        "\n",
        "Since the performance of these two is extremely close, they both represent excellent starting points for our more advanced re-ranking experiments."
      ],
      "metadata": {
        "id": "fAm4TyMngOnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update After Experiment 2:** The Limits of Heuristic Graph Re-ranking\n",
        "The results from the enriched graph re-ranking experiment are in, and they provide a crucial insight: the graph-based re-ranking strategies did not improve performance.\n",
        "\n",
        "Experiment 1 Champion (Hybrid Retriever): nDCG@10 of 0.3950\n",
        "\n",
        "Experiment 2 Best (Graph Re-ranked): nDCG@10 of 0.3400\n",
        "\n",
        "This performance drop is a valuable finding. It suggests that the initial hybrid retrieval stage is already very effective. The simple, additive graph bonuses (parent, citation, etc.) were not nuanced enough to improve upon this strong baseline and, in some cases, likely introduced noise that harmed the ranking.\n",
        "\n",
        "This is not a failure, but a confirmation that to achieve the next level of precision, a more powerful re-ranking method is required. This leads us directly to the next logical step in our plan."
      ],
      "metadata": {
        "id": "6Icy2rh-gRvQ"
      }
    }
  ]
}