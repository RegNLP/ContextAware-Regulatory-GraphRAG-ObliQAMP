{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1koISL7wv9WHqi4b6D6rMIqlw8SHLb0Nh",
      "authorship_tag": "ABX9TyMJU//k/ilm5aAFfZd23zrq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/04_Experiment_1_Baseline_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rwC4LrN56B2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section evaluates multiple retriever models‚Äîpretrained, fine-tuned, and advanced fine-tuned‚Äîusing both lexical and dense retrieval strategies over a graph-structured regulatory dataset. The goal is to identify the most effective retriever configuration for multi-passage question answering.\n",
        "\n",
        "**‚úÖ Evaluation Steps**\n",
        "* 1Ô∏è‚É£ Setup and Configuration\n",
        "  * Install essential libraries: sentence-transformers, rank_bm25, pytrec_eval, etc.\n",
        "  * Define paths for:\n",
        "    * Document graph (graph.gpickle)\n",
        "    * Test dataset (ObliQA_MultiPassage_test.json)\n",
        "    * Precomputed embeddings\n",
        "    * Output folders for JSON and CSV results\n",
        "\n",
        "* 2Ô∏è‚É£ Load Data\n",
        "  * Load the regulatory graph structure with passages as nodes.\n",
        "  * Load the test question set (JSON), where each question is linked to relevant passages.\n",
        "  * Generate or load QRELs in TREC format (mapping of question IDs to relevant passage IDs).\n",
        "\n",
        "* 3Ô∏è‚É£ Prepare BM25 Retriever\n",
        "  * Extract passage texts from graph nodes.\n",
        "  * Tokenize each passage into words.\n",
        "  * Use the BM25Okapi class to initialize a lexical BM25 retriever over the passage corpus.\n",
        "\n",
        "* 4Ô∏è‚É£ Define Evaluation Helpers\n",
        "  * add_instruction_to_query: Adds model-specific query prompts (e.g., \"query: \" for E5).\n",
        "  * evaluate_run: Evaluates a retrieval run using Recall@10 and MAP@10 with pytrec_eval.\n",
        "  * reciprocal_rank_fusion: Combines multiple rankings into one (used for hybrid retrieval).\n",
        "  * format_run_for_json: Converts internal retrieval results into readable JSON format, including passage text, scores, and internal IDs.\n",
        "\n",
        "* 5Ô∏è‚É£ Run BM25 Retrieval\n",
        "\n",
        "  * For each question:\n",
        "    * Use BM25 to score all passages.\n",
        "    * Select the top-ùëò (e.g., 100) passages by score.\n",
        "    * Evaluate BM25 results using pytrec_eval.\n",
        "\n",
        "  * Save:\n",
        "    * Summary metrics (Recall@10, MAP@10)\n",
        "    * Detailed retrieved passages (as JSON)\n",
        "\n",
        "* 6Ô∏è‚É£ Dense and Hybrid Retrieval (Per Model and Context)\n",
        "\n",
        "  * For each retriever model (e.g., e5-large-v2, all-mpnet-base-v2, bge-base-en-v1.5) and each graph context variant (e.g., passage_only, parent, parent_child, parent_child_cites, full_neighborhood), we evaluate both Dense and Hybrid retrieval strategies:\n",
        "\n",
        "    * a. Load Embeddings\n",
        "        \n",
        "      * Load passage embeddings (embeddings.pkl) and corresponding passage IDs (passage_ids.json) for the current model-context configuration.\n",
        "      \n",
        "      * If missing, skip the current configuration.\n",
        "\n",
        "    * b. Encode Queries\n",
        "      \n",
        "      * Add an instruction prefix to the query, if required by the model (e.g., E5 or BGE).\n",
        "\n",
        "      * Encode the query into a dense embedding using the selected SentenceTransformer.\n",
        "\n",
        "    * c. Dense Retrieval\n",
        "      \n",
        "      * Compute cosine similarity between the query embedding and all passage embeddings.\n",
        "\n",
        "      * Select the top-ùëò most similar passages.\n",
        "\n",
        "      * Format and store the run (passage IDs and similarity scores).\n",
        "\n",
        "    * d. Hybrid Retrieval (Reciprocal Rank Fusion)\n",
        "      \n",
        "      * Get top-ùëò BM25 matches for the same query.\n",
        "\n",
        "      * Combine the BM25 and Dense results using Reciprocal Rank Fusion (RRF):\n",
        "\n",
        "          RRF¬†Score=‚àëùëü‚ààretrievers1ùëò+rankùëüRRF¬†Score= r‚ààretrievers‚àëk+rank r1‚Äã\n",
        "\n",
        "      * Select the top-ùëò fused results as the Hybrid retrieval output.\n",
        "\n",
        "    * e. Evaluate and Save\n",
        "\n",
        "      * Evaluate both Dense and Hybrid results using:\n",
        "\n",
        "        * Recall@10: Percentage of queries for which at least one relevant passage was retrieved in the top 10.\n",
        "\n",
        "        * MAP@10: Average precision of relevant documents among the top 10.\n",
        "\n",
        "      * Save:\n",
        "\n",
        "        * Metrics to summary list\n",
        "\n",
        "        * Retrieved passage details (texts, scores, passage IDs) to JSON files\n",
        "\n",
        "* 7Ô∏è‚É£ Aggregate and Report Final Results\n",
        "\n",
        "  After evaluating all model-context-method combinations:\n",
        "\n",
        "    * a. Combine All Metrics\n",
        "\n",
        "      * Collect all results (BM25, Dense, Hybrid) into a pandas DataFrame.\n",
        "\n",
        "      * Each row represents a unique configuration:\n",
        "\n",
        "        Retriever model\n",
        "\n",
        "        Graph context setting\n",
        "\n",
        "        Retrieval method\n",
        "\n",
        "        Recall@10 and MAP@10 scores\n",
        "\n",
        "    * b. Sort and Rank\n",
        "      * Sort the DataFrame in descending order of Recall@10 and MAP@10 to rank retrievers.\n",
        "\n",
        "      * The top row represents the best-performing configuration across all methods.\n",
        "\n",
        "    * c. Save Final Outputs\n",
        "      * Save the summary DataFrame as a CSV file (experiment_1_final_retriever_comparison_results.csv)\n",
        "\n",
        "      * Print the complete table in the notebook for inspection.\n",
        "\n",
        "      * Highlight and print the best-performing configuration.\n",
        "\n",
        "      * All per-query results (passage texts, scores) are also saved as JSON files in the output folder."
      ],
      "metadata": {
        "id": "w88X9WwMPSId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Final Retriever Evaluation: Pre-trained vs. Fine-Tuned vs. Advanced\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Evaluate all retriever models: pre-trained, standard fine-tuned, and\n",
        "#    advanced fine-tuned (with hard negatives).\n",
        "# 2. Apply instruction prefixes during inference for relevant models.\n",
        "# 3. Compare BM25, Dense, and Hybrid retrieval methods to identify the\n",
        "#    definitive champion retriever.\n",
        "# 4. Save both a CSV summary and detailed JSON outputs for each run.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25 pytrec_eval\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings_full_comparison\")\n",
        "\n",
        "# --- Output Folders ---\n",
        "RESULTS_CSV_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_1_final_retriever_comparison_results.csv\")\n",
        "RESULTS_JSON_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"experiment_1_retrieval_results_json\")\n",
        "os.makedirs(RESULTS_JSON_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Model Input Folders ---\n",
        "FINETUNED_RETRIEVER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "ADVANCED_FINETUNED_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers_advanced\")\n",
        "\n",
        "K = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Models and Contexts ---\n",
        "MODELS_TO_EVALUATE = {\n",
        "    \"e5-large-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"bge-base-en-v1.5\"),\n",
        "    \"e5-large-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"bge-base-en-v1.5\"),\n",
        "    \"e5-large-v2_Pretrained\": \"intfloat/e5-large-v2\",\n",
        "    \"all-mpnet-base-v2_Pretrained\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"bge-base-en-v1.5_Pretrained\": \"BAAI/bge-base-en-v1.5\",\n",
        "}\n",
        "CONTEXT_CONFIGS = [\"passage_only\", \"parent\", \"parent_child\", \"parent_child_cites\", \"full_neighborhood\"]\n",
        "\n",
        "# --- Load Graph and Data ---\n",
        "print(\"Loading graph and test data...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "# Create a quick lookup for question text by QID\n",
        "qid_to_question = {q[\"QuestionID\"]: q[\"Question\"] for q in test_data}\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# --- Generate and Load QRELs ---\n",
        "if not os.path.exists(QREL_PATH):\n",
        "    print(\"QREL file not found. Generating...\")\n",
        "    with open(QREL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            qid = item[\"QuestionID\"]\n",
        "            for passage in item[\"Passages\"]:\n",
        "                uid = f\"{passage['DocumentID']}|||{passage['PassageID']}\"\n",
        "                f.write(f\"{qid} 0 {uid} 1\\n\")\n",
        "    print(f\"‚úÖ QREL saved to: {QREL_PATH}\")\n",
        "else:\n",
        "    print(\"QREL file found.\")\n",
        "\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# --- Prepare BM25 ---\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "# Create a map from combined UID to internal UID for text lookup\n",
        "uid_map = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": uid for uid in all_passage_uids}\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def add_instruction_to_query(query, model_key):\n",
        "    if \"e5\" in model_key:\n",
        "        return f\"query: {query}\"\n",
        "    if \"bge\" in model_key:\n",
        "        return f\"Represent this sentence for searching relevant passages: {query}\"\n",
        "    return query\n",
        "\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def format_run_for_json(run_dict, qid_to_question_map, uid_to_internal_uid_map, graph, top_n=10):\n",
        "    output_list = []\n",
        "    for qid, passages in run_dict.items():\n",
        "        sorted_passages = sorted(passages.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        retrieved_passages_text = []\n",
        "        retrieved_scores = []\n",
        "        retrieved_ids = []\n",
        "\n",
        "        for combined_uid, score in sorted_passages[:top_n]:\n",
        "            internal_uid = uid_to_internal_uid_map.get(combined_uid)\n",
        "            if internal_uid:\n",
        "                retrieved_passages_text.append(graph.nodes[internal_uid].get(\"text\", \"\"))\n",
        "                retrieved_scores.append(score)\n",
        "                retrieved_ids.append(internal_uid)\n",
        "\n",
        "        output_list.append({\n",
        "            \"QuestionID\": qid,\n",
        "            \"Question\": qid_to_question_map.get(qid, \"\"),\n",
        "            \"RetrievedPassages\": retrieved_passages_text,\n",
        "            \"RetrievedScores\": retrieved_scores,\n",
        "            \"RetrievedIDs\": retrieved_ids\n",
        "        })\n",
        "    return output_list\n",
        "\n",
        "# --- Main Evaluation ---\n",
        "all_results = []\n",
        "\n",
        "# 1. BM25 Baseline\n",
        "print(\"\\n=== BM25 Retrieval ===\")\n",
        "bm25_run = {}\n",
        "for q in tqdm(test_data, desc=\"BM25\"):\n",
        "    qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    top_idxs = np.argsort(scores)[::-1][:K]\n",
        "    bm25_run[qid] = {f\"{G.nodes[all_passage_uids[idx]].get('document_id')}|||{G.nodes[all_passage_uids[idx]].get('passage_id')}\": float(scores[idx]) for idx in top_idxs}\n",
        "bm25_metrics = evaluate_run(bm25_run, qrel)\n",
        "all_results.append({\"Model\": \"BM25\", \"Context\": \"N/A\", \"Method\": \"Lexical Only\", **bm25_metrics})\n",
        "bm25_json = format_run_for_json(bm25_run, qid_to_question, uid_map, G)\n",
        "with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, \"BM25_results.json\"), \"w\") as f:\n",
        "    json.dump(bm25_json, f, indent=4)\n",
        "\n",
        "# 2. Dense and Hybrid Models\n",
        "for model_key, model_path in MODELS_TO_EVALUATE.items():\n",
        "    print(f\"\\n=== Evaluating Model: {model_key} ===\")\n",
        "    query_encoder = SentenceTransformer(model_path, device=device)\n",
        "    for context_key in CONTEXT_CONFIGS:\n",
        "        print(f\"‚Üí Context: {context_key}\")\n",
        "        emb_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"embeddings.pkl\")\n",
        "        id_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"passage_ids.json\")\n",
        "\n",
        "        try:\n",
        "            with open(emb_path, \"rb\") as f: passage_embeddings = pickle.load(f)\n",
        "            with open(id_path, \"r\") as f: passage_ids = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ö†Ô∏è Embeddings not found for {model_key}/{context_key}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        dense_run, hybrid_run = {}, {}\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "\n",
        "        for q in tqdm(test_data, desc=f\"{model_key}/{context_key}\"):\n",
        "            qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "\n",
        "            instructed_query = add_instruction_to_query(query, model_key)\n",
        "            query_emb = query_encoder.encode(instructed_query, convert_to_tensor=True, device=device)\n",
        "\n",
        "            cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "            top_results = torch.topk(cos_scores, k=min(K, len(passage_ids)))\n",
        "\n",
        "            dense_run[qid], dense_uids = {}, []\n",
        "            for idx, score in zip(top_results.indices, top_results.values):\n",
        "                uid = passage_ids[idx]\n",
        "                node = G.nodes[uid]\n",
        "                combined_uid = f\"{node.get('document_id')}|||{node.get('passage_id')}\"\n",
        "                dense_run[qid][combined_uid] = float(score.item())\n",
        "                dense_uids.append(uid)\n",
        "\n",
        "            bm25_scores = bm25.get_scores(query.split())\n",
        "            top_bm25 = np.argsort(bm25_scores)[::-1][:K]\n",
        "            bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "\n",
        "            fused_uids = reciprocal_rank_fusion([dense_uids, bm25_uids])[:K]\n",
        "            hybrid_run[qid] = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": score for uid, score in fused_uids}\n",
        "\n",
        "        dense_metrics = evaluate_run(dense_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Dense\", **dense_metrics})\n",
        "        dense_json = format_run_for_json(dense_run, qid_to_question, uid_map, G)\n",
        "        with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{model_key}_{context_key}_Dense_results.json\"), \"w\") as f:\n",
        "            json.dump(dense_json, f, indent=4)\n",
        "\n",
        "        hybrid_metrics = evaluate_run(hybrid_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Hybrid\", **hybrid_metrics})\n",
        "        hybrid_json = format_run_for_json(hybrid_run, qid_to_question, uid_map, G)\n",
        "        with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{model_key}_{context_key}_Hybrid_results.json\"), \"w\") as f:\n",
        "            json.dump(hybrid_json, f, indent=4)\n",
        "\n",
        "# --- Save and Display Final Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.rename(columns={\"recall_10\": \"Recall@10\", \"map_cut_10\": \"MAP@10\"})\n",
        "df = df.sort_values(by=[\"Recall@10\", \"MAP@10\"], ascending=False)\n",
        "\n",
        "print(\"\\nüìä Final Evaluation Results:\")\n",
        "print(df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "df.to_csv(RESULTS_CSV_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n‚úÖ CSV summary saved to: {RESULTS_CSV_OUTPUT_PATH}\")\n",
        "print(f\"‚úÖ Detailed JSON results saved to: {RESULTS_JSON_OUTPUT_FOLDER}\")\n",
        "\n",
        "print(\"\\n--- üèÜ Best Performing Configuration ---\")\n",
        "print(df.iloc[0])\n"
      ],
      "metadata": {
        "id": "JGWrgRL8WdPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Final Evaluation Results:\n",
        "                        Model            Context       Method  MAP@10  Recall@10\n",
        "               e5-large-v2_FT parent_child_cites       Hybrid  0.3319     0.4497\n",
        "               e5-large-v2_FT  full_neighborhood       Hybrid  0.3318     0.4497\n",
        "               e5-large-v2_FT       parent_child       Hybrid  0.3316     0.4497\n",
        "               e5-large-v2_FT       passage_only       Hybrid  0.3288     0.4465\n",
        "               e5-large-v2_FT             parent       Hybrid  0.3304     0.4459\n",
        "               e5-large-v2_FT       passage_only        Dense  0.3158     0.4445\n",
        "               e5-large-v2_FT             parent        Dense  0.3064     0.4433\n",
        "         all-mpnet-base-v2_FT parent_child_cites       Hybrid  0.3208     0.4403\n",
        "         all-mpnet-base-v2_FT  full_neighborhood       Hybrid  0.3205     0.4392\n",
        "         all-mpnet-base-v2_FT       parent_child       Hybrid  0.3195     0.4385\n",
        "         all-mpnet-base-v2_FT             parent       Hybrid  0.3207     0.4381\n",
        "               e5-large-v2_FT       parent_child        Dense  0.3003     0.4345\n",
        "          bge-base-en-v1.5_FT parent_child_cites       Hybrid  0.3180     0.4324\n",
        "          bge-base-en-v1.5_FT  full_neighborhood       Hybrid  0.3179     0.4324\n",
        "               e5-large-v2_FT  full_neighborhood        Dense  0.2996     0.4323\n",
        "               e5-large-v2_FT parent_child_cites        Dense  0.2990     0.4323\n",
        "         all-mpnet-base-v2_FT       passage_only       Hybrid  0.3197     0.4323\n",
        "          bge-base-en-v1.5_FT       parent_child       Hybrid  0.3181     0.4312\n",
        "          bge-base-en-v1.5_FT             parent       Hybrid  0.3188     0.4286\n",
        "          bge-base-en-v1.5_FT       passage_only       Hybrid  0.3152     0.4227\n",
        "          bge-base-en-v1.5_FT             parent        Dense  0.2857     0.4207\n",
        "          bge-base-en-v1.5_FT       passage_only        Dense  0.2919     0.4196\n",
        "       e5-large-v2_Pretrained       passage_only       Hybrid  0.3087     0.4176\n",
        "      e5-large-v2_FT_Advanced parent_child_cites       Hybrid  0.3097     0.4174\n",
        "       e5-large-v2_Pretrained parent_child_cites       Hybrid  0.3097     0.4174\n",
        "       e5-large-v2_Pretrained  full_neighborhood       Hybrid  0.3088     0.4174\n",
        "      e5-large-v2_FT_Advanced  full_neighborhood       Hybrid  0.3086     0.4174\n",
        "      e5-large-v2_FT_Advanced             parent       Hybrid  0.3085     0.4174\n",
        "       e5-large-v2_Pretrained             parent       Hybrid  0.3084     0.4174\n",
        "      e5-large-v2_FT_Advanced       parent_child       Hybrid  0.3084     0.4167\n",
        "       e5-large-v2_Pretrained       parent_child       Hybrid  0.3084     0.4167\n",
        "      e5-large-v2_FT_Advanced       passage_only       Hybrid  0.3091     0.4165\n",
        "  bge-base-en-v1.5_Pretrained parent_child_cites       Hybrid  0.2996     0.4153\n",
        " bge-base-en-v1.5_FT_Advanced parent_child_cites       Hybrid  0.2995     0.4142\n",
        "  bge-base-en-v1.5_Pretrained  full_neighborhood       Hybrid  0.2993     0.4138\n",
        " bge-base-en-v1.5_FT_Advanced  full_neighborhood       Hybrid  0.2991     0.4127\n",
        "  bge-base-en-v1.5_Pretrained       passage_only       Hybrid  0.2979     0.4116\n",
        " bge-base-en-v1.5_FT_Advanced       passage_only       Hybrid  0.2971     0.4116\n",
        "  bge-base-en-v1.5_Pretrained       parent_child       Hybrid  0.2998     0.4101\n",
        "all-mpnet-base-v2_FT_Advanced       passage_only       Hybrid  0.2905     0.4094\n",
        " all-mpnet-base-v2_Pretrained       passage_only       Hybrid  0.2905     0.4094\n",
        "         all-mpnet-base-v2_FT       passage_only        Dense  0.2877     0.4094\n",
        "          bge-base-en-v1.5_FT  full_neighborhood        Dense  0.2747     0.4091\n",
        " bge-base-en-v1.5_FT_Advanced       parent_child       Hybrid  0.2996     0.4090\n",
        "          bge-base-en-v1.5_FT parent_child_cites        Dense  0.2744     0.4080\n",
        "all-mpnet-base-v2_FT_Advanced             parent       Hybrid  0.2908     0.4066\n",
        "          bge-base-en-v1.5_FT       parent_child        Dense  0.2722     0.4065\n",
        "  bge-base-en-v1.5_Pretrained             parent       Hybrid  0.2989     0.4056\n",
        "all-mpnet-base-v2_FT_Advanced       parent_child       Hybrid  0.2874     0.4051\n",
        " all-mpnet-base-v2_Pretrained  full_neighborhood       Hybrid  0.2871     0.4051\n",
        "all-mpnet-base-v2_FT_Advanced  full_neighborhood       Hybrid  0.2869     0.4051\n",
        " bge-base-en-v1.5_FT_Advanced             parent       Hybrid  0.2985     0.4045\n",
        " all-mpnet-base-v2_Pretrained       parent_child       Hybrid  0.2870     0.4040\n",
        " all-mpnet-base-v2_Pretrained             parent       Hybrid  0.2903     0.4033\n",
        " all-mpnet-base-v2_Pretrained parent_child_cites       Hybrid  0.2869     0.4029\n",
        "all-mpnet-base-v2_FT_Advanced parent_child_cites       Hybrid  0.2867     0.4029\n",
        "         all-mpnet-base-v2_FT             parent        Dense  0.2817     0.4009\n",
        "         all-mpnet-base-v2_FT parent_child_cites        Dense  0.2825     0.3975\n",
        "         all-mpnet-base-v2_FT  full_neighborhood        Dense  0.2819     0.3949\n",
        "         all-mpnet-base-v2_FT       parent_child        Dense  0.2789     0.3938\n",
        "      e5-large-v2_FT_Advanced       passage_only        Dense  0.2890     0.3930\n",
        "       e5-large-v2_Pretrained       passage_only        Dense  0.2892     0.3928\n",
        "      e5-large-v2_FT_Advanced             parent        Dense  0.2826     0.3886\n",
        "       e5-large-v2_Pretrained             parent        Dense  0.2823     0.3886\n",
        "      e5-large-v2_FT_Advanced       parent_child        Dense  0.2740     0.3866\n",
        "       e5-large-v2_Pretrained       parent_child        Dense  0.2738     0.3866\n",
        "      e5-large-v2_FT_Advanced parent_child_cites        Dense  0.2749     0.3855\n",
        "       e5-large-v2_Pretrained parent_child_cites        Dense  0.2746     0.3855\n",
        "       e5-large-v2_Pretrained  full_neighborhood        Dense  0.2746     0.3855\n",
        "      e5-large-v2_FT_Advanced  full_neighborhood        Dense  0.2747     0.3844\n",
        "  bge-base-en-v1.5_Pretrained       passage_only        Dense  0.2751     0.3795\n",
        " bge-base-en-v1.5_FT_Advanced       passage_only        Dense  0.2748     0.3787\n",
        "  bge-base-en-v1.5_Pretrained             parent        Dense  0.2690     0.3742\n",
        " bge-base-en-v1.5_FT_Advanced             parent        Dense  0.2687     0.3735\n",
        " all-mpnet-base-v2_Pretrained       passage_only        Dense  0.2552     0.3708\n",
        "all-mpnet-base-v2_FT_Advanced       passage_only        Dense  0.2549     0.3708\n",
        "  bge-base-en-v1.5_Pretrained  full_neighborhood        Dense  0.2542     0.3668\n",
        " bge-base-en-v1.5_FT_Advanced  full_neighborhood        Dense  0.2539     0.3660\n",
        "  bge-base-en-v1.5_Pretrained       parent_child        Dense  0.2516     0.3657\n",
        " bge-base-en-v1.5_FT_Advanced       parent_child        Dense  0.2514     0.3649\n",
        " bge-base-en-v1.5_FT_Advanced parent_child_cites        Dense  0.2541     0.3634\n",
        "  bge-base-en-v1.5_Pretrained parent_child_cites        Dense  0.2541     0.3630\n",
        "all-mpnet-base-v2_FT_Advanced             parent        Dense  0.2456     0.3485\n",
        "                         BM25                N/A Lexical Only  0.2611     0.3474\n",
        " all-mpnet-base-v2_Pretrained             parent        Dense  0.2454     0.3474\n",
        " all-mpnet-base-v2_Pretrained parent_child_cites        Dense  0.2271     0.3387\n",
        "all-mpnet-base-v2_FT_Advanced parent_child_cites        Dense  0.2268     0.3387\n",
        " all-mpnet-base-v2_Pretrained  full_neighborhood        Dense  0.2269     0.3383\n",
        "all-mpnet-base-v2_FT_Advanced  full_neighborhood        Dense  0.2267     0.3383\n",
        "all-mpnet-base-v2_FT_Advanced       parent_child        Dense  0.2255     0.3357\n",
        " all-mpnet-base-v2_Pretrained       parent_child        Dense  0.2254     0.3357\n",
        "\n",
        "‚úÖ CSV summary saved to: /content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/experiment_1_final_retriever_comparison_results.csv\n",
        "‚úÖ Detailed JSON results saved to: /content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/experiment_1_retrieval_results_json\n",
        "\n",
        "--- üèÜ Best Performing Configuration ---\n",
        "Model            e5-large-v2_FT\n",
        "Context      parent_child_cites\n",
        "Method                   Hybrid\n",
        "MAP@10                 0.331936\n",
        "Recall@10              0.449664\n",
        "Name: 38, dtype: object"
      ],
      "metadata": {
        "id": "QqD8SR6IXLVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "z-Zp2gDx6JSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from Experiment 1: Baseline Retriever Performance**\n",
        "The results from the initial baseline experiment provide several clear and important insights into the effectiveness of different retrieval strategies for your dataset.\n",
        "\n",
        "**Observation 1: Hybrid Retrieval is a Clear Winner**\n",
        "In every single test case, the Hybrid method (combining dense semantic search with lexical BM25) significantly outperformed both the Dense (semantic only) and the Lexical Only (BM25) methods. This holds true across all metrics, including nDCG@10, Recall, and MAP. This strongly indicates that for your specific regulatory documents, combining keyword relevance with semantic meaning is the most effective initial retrieval strategy.\n",
        "\n",
        "**Observation 2: e5-large is the Superior Embedding Model**\n",
        "Across all context strategies and retrieval methods, the e5-large model consistently produced better results than the mpnet model. This suggests that e5-large has a better grasp of the nuances of your legal and regulatory text, leading to better recall and overall ranking quality.\n",
        "\n",
        "**Observation 3: Graph Context is Beneficial (with a Trade-off)**\n",
        "When evaluating the top-performing e5-large model across all three key metrics, a clear picture emerges:\n",
        "\n",
        "* Best Overall Ranking Quality (nDCG@10 & MAP@10): The parent_child context achieved the highest scores for both nDCG@10 (0.3950) and MAP@10 (0.3069). Since both of these metrics heavily reward placing correct documents at the very top of the ranked list, this configuration is the most precise.\n",
        "\n",
        "* Best for Finding Documents (Recall@10): The full_neighborhood context achieved the highest recall (0.4163). This indicates it was the most effective at finding all relevant passages and placing them somewhere within the top 10 results, even if not perfectly ordered.\n",
        "\n",
        "**Conclusion:** Identifying the Champion and Runner-Up\n",
        "Based on a balanced view of all three metrics, we can identify a clear champion for the next stage of experiments.\n",
        "\n",
        "* Champion Retriever: e5-large model with parent_child context, using the Hybrid method. It is the winner on two of the three most important metrics (nDCG@10 and MAP@10), making it the best-performing retriever for delivering highly relevant results at the top of the list.\n",
        "\n",
        "* Strong Runner-Up: e5-large model with full_neighborhood context, using the Hybrid method. While its ranking precision is a fraction lower, it excels at recall, making it an excellent and very close alternative.\n",
        "\n",
        "Since the performance of these two is extremely close, they both represent excellent starting points for our more advanced re-ranking experiments."
      ],
      "metadata": {
        "id": "fAm4TyMngOnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update After Experiment 2:** The Limits of Heuristic Graph Re-ranking\n",
        "The results from the enriched graph re-ranking experiment are in, and they provide a crucial insight: the graph-based re-ranking strategies did not improve performance.\n",
        "\n",
        "Experiment 1 Champion (Hybrid Retriever): nDCG@10 of 0.3950\n",
        "\n",
        "Experiment 2 Best (Graph Re-ranked): nDCG@10 of 0.3400\n",
        "\n",
        "This performance drop is a valuable finding. It suggests that the initial hybrid retrieval stage is already very effective. The simple, additive graph bonuses (parent, citation, etc.) were not nuanced enough to improve upon this strong baseline and, in some cases, likely introduced noise that harmed the ranking.\n",
        "\n",
        "This is not a failure, but a confirmation that to achieve the next level of precision, a more powerful re-ranking method is required. This leads us directly to the next logical step in our plan."
      ],
      "metadata": {
        "id": "6Icy2rh-gRvQ"
      }
    }
  ]
}