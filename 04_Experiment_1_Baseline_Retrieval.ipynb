{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1koISL7wv9WHqi4b6D6rMIqlw8SHLb0Nh",
      "authorship_tag": "ABX9TyNV7ZS2P+W+GskrUDOcE758",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/04_Experiment_1_Baseline_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rwC4LrN56B2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Baseline Retriever Evaluation: Pre-trained vs. Fine-Tuned vs. Advanced\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Evaluate all retriever models: pre-trained, standard fine-tuned, and\n",
        "#    advanced fine-tuned (with hard negatives).\n",
        "# 2. Apply instruction prefixes during inference for relevant models.\n",
        "# 3. Compare BM25, Dense, and Hybrid retrieval methods to identify the\n",
        "#    definitive champion retriever.\n",
        "# 4. Save both a CSV summary and detailed JSON outputs for each run.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25 pytrec_eval\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings_full_comparison\")\n",
        "\n",
        "# --- Output Folders ---\n",
        "RESULTS_CSV_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_1_final_retriever_comparison_results.csv\")\n",
        "RESULTS_JSON_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"experiment_1_retrieval_results_json\")\n",
        "os.makedirs(RESULTS_JSON_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Model Input Folders ---\n",
        "FINETUNED_RETRIEVER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "ADVANCED_FINETUNED_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers_advanced\")\n",
        "\n",
        "K = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Models and Contexts ---\n",
        "MODELS_TO_EVALUATE = {\n",
        "    \"e5-large-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT_Advanced\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"bge-base-en-v1.5\"),\n",
        "    \"e5-large-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"e5-large-v2\"),\n",
        "    \"all-mpnet-base-v2_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"all-mpnet-base-v2\"),\n",
        "    \"bge-base-en-v1.5_FT\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"bge-base-en-v1.5\"),\n",
        "    \"e5-large-v2_Pretrained\": \"intfloat/e5-large-v2\",\n",
        "    \"all-mpnet-base-v2_Pretrained\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"bge-base-en-v1.5_Pretrained\": \"BAAI/bge-base-en-v1.5\",\n",
        "}\n",
        "CONTEXT_CONFIGS = [\"passage_only\", \"parent\", \"parent_child\", \"parent_child_cites\", \"full_neighborhood\"]\n",
        "\n",
        "# --- Load Graph and Data ---\n",
        "print(\"Loading graph and test data...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "# Create a quick lookup for question text by QID\n",
        "qid_to_question = {q[\"QuestionID\"]: q[\"Question\"] for q in test_data}\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# --- Generate and Load QRELs ---\n",
        "if not os.path.exists(QREL_PATH):\n",
        "    print(\"QREL file not found. Generating...\")\n",
        "    with open(QREL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            qid = item[\"QuestionID\"]\n",
        "            for passage in item[\"Passages\"]:\n",
        "                uid = f\"{passage['DocumentID']}|||{passage['PassageID']}\"\n",
        "                f.write(f\"{qid} 0 {uid} 1\\n\")\n",
        "    print(f\"‚úÖ QREL saved to: {QREL_PATH}\")\n",
        "else:\n",
        "    print(\"QREL file found.\")\n",
        "\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# --- Prepare BM25 ---\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "# Create a map from combined UID to internal UID for text lookup\n",
        "uid_map = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": uid for uid in all_passage_uids}\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def add_instruction_to_query(query, model_key):\n",
        "    if \"e5\" in model_key:\n",
        "        return f\"query: {query}\"\n",
        "    if \"bge\" in model_key:\n",
        "        return f\"Represent this sentence for searching relevant passages: {query}\"\n",
        "    return query\n",
        "\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\", \"ndcg_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "def format_run_for_json(run_dict, qid_to_question_map, uid_to_internal_uid_map, graph, top_n=10):\n",
        "    output_list = []\n",
        "    for qid, passages in run_dict.items():\n",
        "        sorted_passages = sorted(passages.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        retrieved_passages_text = []\n",
        "        retrieved_scores = []\n",
        "        retrieved_ids = []\n",
        "\n",
        "        # CORRECTED: Only take the top_n results for the JSON output\n",
        "        for combined_uid, score in sorted_passages[:top_n]:\n",
        "            internal_uid = uid_to_internal_uid_map.get(combined_uid)\n",
        "            if internal_uid:\n",
        "                retrieved_passages_text.append(graph.nodes[internal_uid].get(\"text\", \"\"))\n",
        "                retrieved_scores.append(score)\n",
        "                retrieved_ids.append(internal_uid) # Using internal UID as the primary ID\n",
        "\n",
        "        output_list.append({\n",
        "            \"QuestionID\": qid,\n",
        "            \"Question\": qid_to_question_map.get(qid, \"\"),\n",
        "            \"RetrievedPassages\": retrieved_passages_text,\n",
        "            \"RetrievedScores\": retrieved_scores,\n",
        "            \"RetrievedIDs\": retrieved_ids\n",
        "        })\n",
        "    return output_list\n",
        "\n",
        "# --- Main Evaluation ---\n",
        "all_results = []\n",
        "\n",
        "# 1. BM25 Baseline\n",
        "print(\"\\n=== BM25 Retrieval ===\")\n",
        "bm25_run = {}\n",
        "for q in tqdm(test_data, desc=\"BM25\"):\n",
        "    qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    top_idxs = np.argsort(scores)[::-1][:K]\n",
        "    bm25_run[qid] = {f\"{G.nodes[all_passage_uids[idx]].get('document_id')}|||{G.nodes[all_passage_uids[idx]].get('passage_id')}\": float(scores[idx]) for idx in top_idxs}\n",
        "bm25_metrics = evaluate_run(bm25_run, qrel)\n",
        "all_results.append({\"Model\": \"BM25\", \"Context\": \"N/A\", \"Method\": \"Lexical Only\", **bm25_metrics})\n",
        "# Save JSON output for BM25\n",
        "bm25_json = format_run_for_json(bm25_run, qid_to_question, uid_map, G)\n",
        "with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, \"BM25_results.json\"), \"w\") as f:\n",
        "    json.dump(bm25_json, f, indent=4)\n",
        "\n",
        "# 2. Dense and Hybrid Models\n",
        "for model_key, model_path in MODELS_TO_EVALUATE.items():\n",
        "    print(f\"\\n=== Evaluating Model: {model_key} ===\")\n",
        "    query_encoder = SentenceTransformer(model_path, device=device)\n",
        "    for context_key in CONTEXT_CONFIGS:\n",
        "        print(f\"‚Üí Context: {context_key}\")\n",
        "        emb_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"embeddings.pkl\")\n",
        "        id_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"passage_ids.json\")\n",
        "\n",
        "        try:\n",
        "            with open(emb_path, \"rb\") as f: passage_embeddings = pickle.load(f)\n",
        "            with open(id_path, \"r\") as f: passage_ids = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ö†Ô∏è Embeddings not found for {model_key}/{context_key}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        dense_run, hybrid_run = {}, {}\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "\n",
        "        for q in tqdm(test_data, desc=f\"{model_key}/{context_key}\"):\n",
        "            qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "\n",
        "            instructed_query = add_instruction_to_query(query, model_key)\n",
        "            query_emb = query_encoder.encode(instructed_query, convert_to_tensor=True, device=device)\n",
        "\n",
        "            cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "            top_results = torch.topk(cos_scores, k=min(K, len(passage_ids)))\n",
        "\n",
        "            dense_run[qid], dense_uids = {}, []\n",
        "            for idx, score in zip(top_results.indices, top_results.values):\n",
        "                uid = passage_ids[idx]\n",
        "                node = G.nodes[uid]\n",
        "                combined_uid = f\"{node.get('document_id')}|||{node.get('passage_id')}\"\n",
        "                dense_run[qid][combined_uid] = float(score.item())\n",
        "                dense_uids.append(uid)\n",
        "\n",
        "            bm25_scores = bm25.get_scores(query.split())\n",
        "            top_bm25 = np.argsort(bm25_scores)[::-1][:K]\n",
        "            bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "\n",
        "            fused_uids = reciprocal_rank_fusion([dense_uids, bm25_uids])[:K]\n",
        "            hybrid_run[qid] = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": score for uid, score in fused_uids}\n",
        "\n",
        "        # Evaluate and save results for Dense run\n",
        "        dense_metrics = evaluate_run(dense_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Dense\", **dense_metrics})\n",
        "        dense_json = format_run_for_json(dense_run, qid_to_question, uid_map, G)\n",
        "        with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{model_key}_{context_key}_Dense_results.json\"), \"w\") as f:\n",
        "            json.dump(dense_json, f, indent=4)\n",
        "\n",
        "        # Evaluate and save results for Hybrid run\n",
        "        hybrid_metrics = evaluate_run(hybrid_run, qrel)\n",
        "        all_results.append({\"Model\": model_key, \"Context\": context_key, \"Method\": \"Hybrid\", **hybrid_metrics})\n",
        "        hybrid_json = format_run_for_json(hybrid_run, qid_to_question, uid_map, G)\n",
        "        with open(os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{model_key}_{context_key}_Hybrid_results.json\"), \"w\") as f:\n",
        "            json.dump(hybrid_json, f, indent=4)\n",
        "\n",
        "# --- Save and Display Final Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.rename(columns={\"recall_10\": \"Recall@10\", \"map_cut_10\": \"MAP@10\"})\n",
        "df = df.sort_values(by=\"nDCG@10\", ascending=False)\n",
        "\n",
        "print(\"\\nüìä Final Evaluation Results:\")\n",
        "print(df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "df.to_csv(RESULTS_CSV_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n‚úÖ CSV summary saved to: {RESULTS_CSV_OUTPUT_PATH}\")\n",
        "print(f\"‚úÖ Detailed JSON results saved to: {RESULTS_JSON_OUTPUT_FOLDER}\")\n",
        "\n",
        "print(\"\\n--- üèÜ Best Performing Configuration ---\")\n",
        "print(df.iloc[0])\n"
      ],
      "metadata": {
        "id": "JGWrgRL8WdPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "z-Zp2gDx6JSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from Experiment 1: Baseline Retriever Performance**\n",
        "The results from the initial baseline experiment provide several clear and important insights into the effectiveness of different retrieval strategies for your dataset.\n",
        "\n",
        "**Observation 1: Hybrid Retrieval is a Clear Winner**\n",
        "In every single test case, the Hybrid method (combining dense semantic search with lexical BM25) significantly outperformed both the Dense (semantic only) and the Lexical Only (BM25) methods. This holds true across all metrics, including nDCG@10, Recall, and MAP. This strongly indicates that for your specific regulatory documents, combining keyword relevance with semantic meaning is the most effective initial retrieval strategy.\n",
        "\n",
        "**Observation 2: e5-large is the Superior Embedding Model**\n",
        "Across all context strategies and retrieval methods, the e5-large model consistently produced better results than the mpnet model. This suggests that e5-large has a better grasp of the nuances of your legal and regulatory text, leading to better recall and overall ranking quality.\n",
        "\n",
        "**Observation 3: Graph Context is Beneficial (with a Trade-off)**\n",
        "When evaluating the top-performing e5-large model across all three key metrics, a clear picture emerges:\n",
        "\n",
        "* Best Overall Ranking Quality (nDCG@10 & MAP@10): The parent_child context achieved the highest scores for both nDCG@10 (0.3950) and MAP@10 (0.3069). Since both of these metrics heavily reward placing correct documents at the very top of the ranked list, this configuration is the most precise.\n",
        "\n",
        "* Best for Finding Documents (Recall@10): The full_neighborhood context achieved the highest recall (0.4163). This indicates it was the most effective at finding all relevant passages and placing them somewhere within the top 10 results, even if not perfectly ordered.\n",
        "\n",
        "**Conclusion:** Identifying the Champion and Runner-Up\n",
        "Based on a balanced view of all three metrics, we can identify a clear champion for the next stage of experiments.\n",
        "\n",
        "* Champion Retriever: e5-large model with parent_child context, using the Hybrid method. It is the winner on two of the three most important metrics (nDCG@10 and MAP@10), making it the best-performing retriever for delivering highly relevant results at the top of the list.\n",
        "\n",
        "* Strong Runner-Up: e5-large model with full_neighborhood context, using the Hybrid method. While its ranking precision is a fraction lower, it excels at recall, making it an excellent and very close alternative.\n",
        "\n",
        "Since the performance of these two is extremely close, they both represent excellent starting points for our more advanced re-ranking experiments."
      ],
      "metadata": {
        "id": "fAm4TyMngOnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Update After Experiment 2:** The Limits of Heuristic Graph Re-ranking\n",
        "The results from the enriched graph re-ranking experiment are in, and they provide a crucial insight: the graph-based re-ranking strategies did not improve performance.\n",
        "\n",
        "Experiment 1 Champion (Hybrid Retriever): nDCG@10 of 0.3950\n",
        "\n",
        "Experiment 2 Best (Graph Re-ranked): nDCG@10 of 0.3400\n",
        "\n",
        "This performance drop is a valuable finding. It suggests that the initial hybrid retrieval stage is already very effective. The simple, additive graph bonuses (parent, citation, etc.) were not nuanced enough to improve upon this strong baseline and, in some cases, likely introduced noise that harmed the ranking.\n",
        "\n",
        "This is not a failure, but a confirmation that to achieve the next level of precision, a more powerful re-ranking method is required. This leads us directly to the next logical step in our plan."
      ],
      "metadata": {
        "id": "6Icy2rh-gRvQ"
      }
    }
  ]
}