{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "18GORChlLoptpeltTdqKlYkJc-bQb9GTe",
      "authorship_tag": "ABX9TyPUCu+JhMHBObZbBVp4V3gM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/06_Experiment_3_Cross_Encoder_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q-Ou_2ej8sI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Experiment 3: Cross Encoder Pipeline Evaluation\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Use the best pre-trained, standard fine-tuned, and advanced fine-tuned\n",
        "#    hybrid retrievers to generate initial candidate passages.\n",
        "# 2. Re-rank these candidates using both fine-tuned and pre-trained\n",
        "#    Cross-Encoder models.\n",
        "# 3. Evaluate the final results to identify the definitive best-performing\n",
        "#    end-to-end pipeline.\n",
        "# 4. Save both a CSV summary and detailed JSON outputs for each run.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25 pytrec_eval\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "\n",
        "# --- Model & Embedding Input Folders ---\n",
        "FINETUNED_RETRIEVER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "ADVANCED_FINETUNED_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers_advanced\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings_full_comparison\")\n",
        "CROSS_ENCODER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_cross_encoders\")\n",
        "\n",
        "# --- Output Paths ---\n",
        "RESULTS_CSV_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_3_final_pipeline_results.csv\")\n",
        "RESULTS_JSON_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"experiment_3_retrieval_results_json\")\n",
        "os.makedirs(RESULTS_JSON_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Champion Retriever Configurations (from new Experiment 1) ---\n",
        "# We select the best context for each type of model\n",
        "CHAMPION_CONFIGS = [\n",
        "    {\"name\": \"e5-large-v2_FT_Advanced_parent\", \"model_path\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"e5-large-v2\"), \"context_key\": \"parent\"},\n",
        "    {\"name\": \"all-mpnet-base-v2_FT_Advanced_parent\", \"model_path\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"all-mpnet-base-v2\"), \"context_key\": \"parent\"},\n",
        "    {\"name\": \"bge-base-en-v1.5_FT_Advanced_parent_child\", \"model_path\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"bge-base-en-v1.5\"), \"context_key\": \"parent_child\"},\n",
        "    {\"name\": \"e5-large-v2_FT_parent\", \"model_path\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"e5-large-v2\"), \"context_key\": \"parent\"},\n",
        "    {\"name\": \"e5-large-v2_Pretrained_parent_child\", \"model_path\": \"intfloat/e5-large-v2\", \"context_key\": \"parent_child\"},\n",
        "]\n",
        "\n",
        "# --- Cross-Encoder Models to Evaluate ---\n",
        "CROSS_ENCODERS_TO_EVALUATE = {\n",
        "    \"MiniLM_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MiniLM_CrossEncoder\"),\n",
        "    \"MPNet_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MPNet_CrossEncoder\"),\n",
        "    \"MSMarco_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MSMarco_CrossEncoder\"),\n",
        "    \"BERT_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"BERT_CrossEncoder\"),\n",
        "    \"MiniLM_Pretrained\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    \"MSMarco_Pretrained\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\",\n",
        "    \"BERT_Pretrained\": \"bert-base-uncased\"\n",
        "}\n",
        "\n",
        "# --- Experiment Parameters ---\n",
        "K_INITIAL = 100\n",
        "K_RERANK = 25\n",
        "K_FINAL = 20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Load Static Components ---\n",
        "print(\"Loading static components...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f: G = pickle.load(f)\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f: test_data = json.load(f)\n",
        "qid_to_question = {q[\"QuestionID\"]: q[\"Question\"] for q in test_data}\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# Load QRELs\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# Prepare BM25\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "uid_map = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": uid for uid in all_passage_uids}\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def add_instruction_to_query(query, model_name):\n",
        "    if \"e5\" in model_name or \"bge\" in model_name:\n",
        "        return f\"query: {query}\"\n",
        "    return query\n",
        "\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\", \"ndcg_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.keys(), key=lambda item: fused[item], reverse=True)\n",
        "\n",
        "def format_run_for_json(run_dict, qid_to_question_map, uid_to_internal_uid_map, graph, top_n=10):\n",
        "    output_list = []\n",
        "    for qid, passages in run_dict.items():\n",
        "        sorted_passages = sorted(passages.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        retrieved_passages_text, retrieved_scores, retrieved_ids = [], [], []\n",
        "        for combined_uid, score in sorted_passages[:top_n]:\n",
        "            internal_uid = uid_to_internal_uid_map.get(combined_uid)\n",
        "            if internal_uid:\n",
        "                retrieved_passages_text.append(graph.nodes[internal_uid].get(\"text\", \"\"))\n",
        "                retrieved_scores.append(score)\n",
        "                retrieved_ids.append(internal_uid)\n",
        "\n",
        "        output_list.append({\n",
        "            \"QuestionID\": qid, \"Question\": qid_to_question_map.get(qid, \"\"),\n",
        "            \"RetrievedPassages\": retrieved_passages_text, \"RetrievedScores\": retrieved_scores,\n",
        "            \"RetrievedIDs\": retrieved_ids\n",
        "        })\n",
        "    return output_list\n",
        "\n",
        "# --- Main Experiment Loop ---\n",
        "all_results = []\n",
        "\n",
        "for config in CHAMPION_CONFIGS:\n",
        "    champion_name = config[\"name\"]\n",
        "    model_path = config[\"model_path\"]\n",
        "    context_key = config[\"context_key\"]\n",
        "    model_key = \"_\".join(champion_name.split('_')[:-1]) # e.g., e5-large-v2_FT_Advanced\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- TESTING CHAMPION RETRIEVER: {champion_name} ---\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load Embeddings & Query Encoder\n",
        "    print(f\"Loading embeddings from: {EMBEDDINGS_FOLDER}\")\n",
        "    emb_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"embeddings.pkl\")\n",
        "    id_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"passage_ids.json\")\n",
        "    try:\n",
        "        with open(emb_path, \"rb\") as f: passage_embeddings = pickle.load(f)\n",
        "        with open(id_path, \"r\") as f: passage_ids = json.load(f)\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "        query_encoder = SentenceTransformer(model_path, device=device)\n",
        "        print(\"Champion retriever components loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"FATAL ERROR: Embeddings not found at {emb_path}. Skipping this champion.\")\n",
        "        continue\n",
        "\n",
        "    # Pre-calculate initial retrievals\n",
        "    print(\"Pre-calculating initial hybrid retrievals...\")\n",
        "    initial_retrievals = {}\n",
        "    for q in tqdm(test_data, desc=f\"Initial Retrieval for {champion_name}\"):\n",
        "        qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "        instructed_query = add_instruction_to_query(query, champion_name)\n",
        "        query_emb = query_encoder.encode(instructed_query, convert_to_tensor=True, device=device)\n",
        "        cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "        top_dense = torch.topk(cos_scores, k=min(K_INITIAL, len(passage_ids)))\n",
        "        dense_uids = [passage_ids[idx] for idx in top_dense.indices]\n",
        "\n",
        "        bm25_scores = bm25.get_scores(query.split())\n",
        "        top_bm25 = np.argsort(bm25_scores)[::-1][:K_INITIAL]\n",
        "        bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "\n",
        "        fused_uids = reciprocal_rank_fusion([dense_uids, bm25_uids])\n",
        "        initial_retrievals[qid] = fused_uids[:K_RERANK]\n",
        "\n",
        "    for ce_name, ce_path in CROSS_ENCODERS_TO_EVALUATE.items():\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"--- Evaluating Cross-Encoder: {ce_name} (with {champion_name}) ---\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        try:\n",
        "            ce_tokenizer = AutoTokenizer.from_pretrained(ce_path)\n",
        "            ce_model = AutoModelForSequenceClassification.from_pretrained(ce_path).to(device)\n",
        "            ce_model.eval()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Could not load model from {ce_path}. Skipping. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "        final_run = {}\n",
        "        for q in tqdm(test_data, desc=f\"Re-ranking with {ce_name}\"):\n",
        "            qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "            candidates_uids = initial_retrievals[qid]\n",
        "\n",
        "            ce_input_pairs = [[query, G.nodes[uid].get(\"text\", \"\")] for uid in candidates_uids]\n",
        "\n",
        "            reranked_candidates = []\n",
        "            with torch.no_grad():\n",
        "                inputs = ce_tokenizer(ce_input_pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "                logits = ce_model(**inputs).logits\n",
        "\n",
        "                if logits.shape[1] > 1:\n",
        "                    scores = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "                else:\n",
        "                    scores = logits.squeeze().cpu().numpy()\n",
        "\n",
        "            # Handle case where only one candidate is passed\n",
        "            if scores.ndim == 0:\n",
        "                scores = [scores.item()]\n",
        "\n",
        "            for i, uid in enumerate(candidates_uids):\n",
        "                reranked_candidates.append({\"internal_uid\": uid, \"score\": scores[i]})\n",
        "\n",
        "            reranked_candidates = sorted(reranked_candidates, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "            final_run[qid] = {}\n",
        "            for cand in reranked_candidates[:K_FINAL]:\n",
        "                node = G.nodes[cand[\"internal_uid\"]]\n",
        "                combined_uid = f\"{node.get('document_id')}|||{node.get('passage_id')}\"\n",
        "                final_run[qid][combined_uid] = float(cand[\"score\"])\n",
        "\n",
        "        ce_metrics = evaluate_run(final_run, qrel)\n",
        "        all_results.append({\n",
        "            \"Retriever\": champion_name, \"Cross-Encoder\": ce_name, **ce_metrics\n",
        "        })\n",
        "\n",
        "        # Save JSON output for this run\n",
        "        json_output_path = os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{champion_name}_{ce_name}_results.json\")\n",
        "        json_data = format_run_for_json(final_run, qid_to_question, uid_map, G)\n",
        "        with open(json_output_path, 'w') as f:\n",
        "            json.dump(json_data, f, indent=4)\n",
        "\n",
        "# --- Save and Display Final Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.rename(columns={\"recall_10\": \"Recall@10\", \"map_cut_10\": \"MAP@10\"})\n",
        "df = df.sort_values(by=[\"Recall@10\", \"MAP@10\"], ascending=False)\n",
        "\n",
        "print(\"\\n📊 Final Evaluation Results:\")\n",
        "print(df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "df.to_csv(RESULTS_CSV_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n✅ CSV summary saved to: {RESULTS_CSV_OUTPUT_PATH}\")\n",
        "print(f\"✅ Detailed JSON results saved to: {RESULTS_JSON_OUTPUT_FOLDER}\")\n",
        "\n",
        "print(\"\\n--- 🏆 Best Performing Full Pipeline ---\")\n",
        "print(df.iloc[0])\n"
      ]
    }
  ]
}