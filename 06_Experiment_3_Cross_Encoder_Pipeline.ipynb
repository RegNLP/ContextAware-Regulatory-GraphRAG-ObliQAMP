{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "18GORChlLoptpeltTdqKlYkJc-bQb9GTe",
      "authorship_tag": "ABX9TyP4YyBIMd0eV8dLZivfuo/Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/06_Experiment_3_Cross_Encoder_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q-Ou_2ej8sI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Experiment 3: Full Pipeline with Cross-Encoders\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Use the top two champion hybrid retrievers from Experiment 1 to generate\n",
        "#    initial sets of candidate passages.\n",
        "# 2. Re-rank these candidates using both fine-tuned and pre-trained\n",
        "#    Cross-Encoder models.\n",
        "# 3. Evaluate the final results to identify the best-performing end-to-end\n",
        "#    pipeline and measure the impact of fine-tuning.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q sentence-transformers pytrec_eval rank_bm25 pandas networkx transformers\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings\")\n",
        "RESULTS_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_3_cross_encoder_results_comparison.csv\")\n",
        "\n",
        "# --- Champion Retriever Configurations (from Experiment 1) ---\n",
        "CHAMPION_CONFIGS = [\n",
        "    {\n",
        "        \"name\": \"e5-large-parent_child\",\n",
        "        \"model_key\": \"e5-large\",\n",
        "        \"model_path\": \"intfloat/e5-large-v2\",\n",
        "        \"context_key\": \"parent_child\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"e5-large-full_neighborhood\",\n",
        "        \"model_key\": \"e5-large\",\n",
        "        \"model_path\": \"intfloat/e5-large-v2\",\n",
        "        \"context_key\": \"full_neighborhood\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Cross-Encoder Models to Evaluate ---\n",
        "CROSS_ENCODER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_cross_encoders\")\n",
        "CROSS_ENCODERS_TO_EVALUATE = {\n",
        "    # Fine-tuned models\n",
        "    \"MiniLM_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MiniLM_CrossEncoder\"),\n",
        "    \"MPNet_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MPNet_CrossEncoder\"),\n",
        "    \"MSMarco_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MSMarco_CrossEncoder\"),\n",
        "    \"BERT_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"BERT_CrossEncoder\"),\n",
        "    # Pre-trained (original) models for comparison\n",
        "    \"MiniLM_Pretrained\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    \"MPNet_Pretrained\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    #\"MSMarco_Pretrained\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\",\n",
        "    \"BERT_Pretrained\": \"bert-base-uncased\"\n",
        "}\n",
        "\n",
        "\n",
        "# --- Experiment Parameters ---\n",
        "K_INITIAL = 100  # Number of initial candidates from hybrid retrieval\n",
        "K_RERANK = 25    # Number of candidates to pass to the cross-encoder\n",
        "K_FINAL = 20     # Final number of results to evaluate\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Load Static Components ---\n",
        "print(\"Loading static components...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# Load QRELs\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# Prepare BM25\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\", \"ndcg_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.keys(), key=lambda item: fused[item], reverse=True)\n",
        "\n",
        "# --- Main Experiment Loop ---\n",
        "all_results = []\n",
        "\n",
        "for champion_config in CHAMPION_CONFIGS:\n",
        "    champion_name = champion_config[\"name\"]\n",
        "    champion_model_key = champion_config[\"model_key\"]\n",
        "    champion_model_path = champion_config[\"model_path\"]\n",
        "    champion_context_key = champion_config[\"context_key\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- TESTING CHAMPION RETRIEVER: {champion_name} ---\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # --- Load Champion Embeddings & Query Encoder ---\n",
        "    print(f\"Loading champion retriever: {champion_model_key} / {champion_context_key}\")\n",
        "    emb_path = os.path.join(EMBEDDINGS_FOLDER, champion_model_key, champion_context_key, \"embeddings.pkl\")\n",
        "    id_path = os.path.join(EMBEDDINGS_FOLDER, champion_model_key, champion_context_key, \"passage_ids.json\")\n",
        "    try:\n",
        "        with open(emb_path, \"rb\") as f:\n",
        "            passage_embeddings = pickle.load(f)\n",
        "        with open(id_path, \"r\") as f:\n",
        "            passage_ids = json.load(f)\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "        query_encoder = SentenceTransformer(champion_model_path, device=device)\n",
        "        print(\"Champion retriever loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"FATAL ERROR: Champion embeddings not found at {emb_path}. Skipping this champion.\")\n",
        "        continue\n",
        "\n",
        "    # --- Pre-calculate initial retrievals for all queries ---\n",
        "    print(\"\\nPre-calculating initial hybrid retrievals for all queries...\")\n",
        "    initial_retrievals = {}\n",
        "    for q in tqdm(test_data, desc=f\"Initial Hybrid Retrieval for {champion_name}\"):\n",
        "        qid = q[\"QuestionID\"]\n",
        "        query = q[\"Question\"]\n",
        "        # Dense retrieval\n",
        "        query_emb = query_encoder.encode(query, convert_to_tensor=True, device=device)\n",
        "        cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "        top_dense = torch.topk(cos_scores, k=min(K_INITIAL, len(passage_ids)))\n",
        "        dense_uids = [passage_ids[idx] for idx in top_dense.indices]\n",
        "        # Lexical retrieval\n",
        "        bm25_scores = bm25.get_scores(query.split())\n",
        "        top_bm25 = np.argsort(bm25_scores)[::-1][:K_INITIAL]\n",
        "        bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "        # Fusion\n",
        "        fused_uids = reciprocal_rank_fusion([dense_uids, bm25_uids])\n",
        "        initial_retrievals[qid] = fused_uids[:K_RERANK]\n",
        "\n",
        "    for ce_name, ce_path in CROSS_ENCODERS_TO_EVALUATE.items():\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"--- Evaluating Cross-Encoder: {ce_name} (with {champion_name}) ---\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        try:\n",
        "            ce_tokenizer = AutoTokenizer.from_pretrained(ce_path)\n",
        "            ce_model = AutoModelForSequenceClassification.from_pretrained(ce_path).to(device)\n",
        "            ce_model.eval()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not load model from {ce_path}. Skipping. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "        final_run = {}\n",
        "        for q in tqdm(test_data, desc=f\"Re-ranking with {ce_name}\"):\n",
        "            qid = q[\"QuestionID\"]\n",
        "            query = q[\"Question\"]\n",
        "\n",
        "            candidates_uids = initial_retrievals[qid]\n",
        "\n",
        "            # --- Stage 2: Cross-Encoder Re-ranking ---\n",
        "            ce_input_pairs = [[query, G.nodes[uid].get(\"text\", \"\")] for uid in candidates_uids]\n",
        "\n",
        "            reranked_candidates = []\n",
        "            with torch.no_grad():\n",
        "                inputs = ce_tokenizer(ce_input_pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "                logits = ce_model(**inputs).logits\n",
        "\n",
        "                # CORRECTED: Handle both regression (1 output) and classification (2 outputs)\n",
        "                if logits.shape[1] > 1:\n",
        "                    # Classification model: get the score for the \"relevant\" class (index 1)\n",
        "                    scores = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "                else:\n",
        "                    # Regression model: use the single output score directly\n",
        "                    scores = logits.squeeze().cpu().numpy()\n",
        "\n",
        "            for i, uid in enumerate(candidates_uids):\n",
        "                reranked_candidates.append({\"internal_uid\": uid, \"score\": scores[i]})\n",
        "\n",
        "            # Sort by the new cross-encoder score\n",
        "            reranked_candidates = sorted(reranked_candidates, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "            # Format for evaluation\n",
        "            final_run[qid] = {}\n",
        "            for cand in reranked_candidates[:K_FINAL]:\n",
        "                node = G.nodes[cand[\"internal_uid\"]]\n",
        "                doc_id = node.get(\"document_id\", \"\")\n",
        "                passage_id = node.get(\"passage_id\", \"\")\n",
        "                if doc_id and passage_id:\n",
        "                    combined_uid = f\"{doc_id}|||{passage_id}\"\n",
        "                    final_run[qid][combined_uid] = float(cand[\"score\"])\n",
        "\n",
        "        # Evaluate and store results for this cross-encoder\n",
        "        ce_metrics = evaluate_run(final_run, qrel)\n",
        "        all_results.append({\n",
        "            \"Retriever\": champion_name,\n",
        "            \"Cross-Encoder\": ce_name,\n",
        "            \"Recall@10\": ce_metrics[\"recall_10\"],\n",
        "            \"MAP@10\": ce_metrics[\"map_cut_10\"],\n",
        "            \"nDCG@10\": ce_metrics[\"ndcg_cut_10\"]\n",
        "        })\n",
        "\n",
        "# --- Save and Display Final Results ---\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df = results_df.sort_values(by=\"nDCG@10\", ascending=False)\n",
        "\n",
        "print(\"\\nüìä Final Evaluation Results:\")\n",
        "print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "results_df.to_csv(RESULTS_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n‚úÖ Results saved to: {RESULTS_OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\n--- üèÜ Best Performing Full Pipeline ---\")\n",
        "print(results_df.iloc[0])\n"
      ]
    }
  ]
}