{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "18GORChlLoptpeltTdqKlYkJc-bQb9GTe",
      "authorship_tag": "ABX9TyMQhk0VUpfnAeYAbiMAAy28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/06_Experiment_3_Cross_Encoder_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M44s_Ywrdc-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è Experiment 3: Full Pipeline Evaluation with Cross-Encoder Re-ranking\n",
        "This experiment identifies the best-performing end-to-end pipeline for multi-passage question answering over regulatory documents. It builds on the strongest hybrid retrievers from Experiment 1 and applies Cross-Encoder re-ranking to further optimize the top retrieved passages.\n",
        "\n",
        "‚úÖ Evaluation Steps\n",
        "1Ô∏è‚É£ Setup and Configuration\n",
        "* Install required packages: sentence-transformers, transformers, rank_bm25, pytrec_eval, etc.\n",
        "\n",
        "* Define input/output paths for:\n",
        "\n",
        "  * Graph structure (graph.gpickle)\n",
        "\n",
        "  * Test dataset (ObliQA_MultiPassage_test.json)\n",
        "\n",
        "  * Fine-tuned retrievers and cross-encoders\n",
        "\n",
        "  * Precomputed embeddings\n",
        "\n",
        "  * Output folders for CSV and JSON files\n",
        "\n",
        "2Ô∏è‚É£ Load Static Components\n",
        "Load the regulatory graph and extract passage nodes.\n",
        "\n",
        "Load the test question set.\n",
        "\n",
        "Load gold standard QRELs in TREC format.\n",
        "\n",
        "Build the BM25 index over all passage texts.\n",
        "\n",
        "3Ô∏è‚É£ Champion Retriever Selection\n",
        "Use two top hybrid retrievers from Experiment 1:\n",
        "\n",
        "e5-large-v2_FT_parent_child_cites\n",
        "\n",
        "e5-large-v2_FT_Advanced_parent\n",
        "\n",
        "These retrievers generate initial top-100 candidates using hybrid retrieval (dense + BM25 + Reciprocal Rank Fusion).\n",
        "\n",
        "4Ô∏è‚É£ Initial Retrieval\n",
        "For each question:\n",
        "\n",
        "Encode query using the retriever‚Äôs embedding model (with prefix like \"query: \").\n",
        "\n",
        "Compute cosine similarity with passage embeddings.\n",
        "\n",
        "Retrieve top-100 dense matches.\n",
        "\n",
        "Retrieve top-100 BM25 matches.\n",
        "\n",
        "Fuse rankings using Reciprocal Rank Fusion (RRF).\n",
        "\n",
        "Keep top-25 fused candidates for re-ranking.\n",
        "\n",
        "5Ô∏è‚É£ Cross-Encoder Re-ranking\n",
        "For each Cross-Encoder model (both fine-tuned and pretrained):\n",
        "\n",
        "* Score each of the 25 candidate passages using the Cross-Encoder model.\n",
        "\n",
        "* Keep top-20 passages based on relevance scores.\n",
        "\n",
        "Evaluate with:\n",
        "\n",
        "* Recall@10: % of questions with at least 1 correct passage in top 10\n",
        "\n",
        "* MAP@10: Mean average precision of top 10 results\n",
        "\n",
        "Cross-Encoder models evaluated:\n",
        "\n",
        "* ‚úÖ Fine-tuned: MiniLM_FT, MPNet_FT, MSMarco_FT, BERT_FT\n",
        "\n",
        "* üß™ Pretrained: MiniLM, MSMarco, BERT\n",
        "\n",
        "6Ô∏è‚É£ Save Results\n",
        "Save each re-ranked run as a JSON file (passages, scores, IDs).\n",
        "\n",
        "Aggregate metrics across all configurations into a summary CSV.\n",
        "\n",
        "Sort results by Recall@10 and MAP@10 to identify the top pipeline.\n",
        "\n",
        "üéØ Objective\n",
        "To pinpoint the most effective retriever + re-ranker combination for regulatory QA ‚Äî demonstrating the impact of learning-based re-ranking over strong graph-aware retrievers."
      ],
      "metadata": {
        "id": "f405kyrTeHjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q-Ou_2ej8sI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Experiment 3: Final Pipeline Evaluation\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Use the best pre-trained, standard fine-tuned, and advanced fine-tuned\n",
        "#    hybrid retrievers to generate initial candidate passages.\n",
        "# 2. Re-rank these candidates using both fine-tuned and pre-trained\n",
        "#    Cross-Encoder models.\n",
        "# 3. Evaluate the final results to identify the definitive best-performing\n",
        "#    end-to-end pipeline.\n",
        "# 4. Save both a CSV summary and detailed JSON outputs for each run.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25 pytrec_eval\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pytrec_eval\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "GRAPH_PATH = os.path.join(BASE_PATH, \"graph.gpickle\")\n",
        "TEST_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_test.json\")\n",
        "QREL_PATH = os.path.join(BASE_PATH, \"qrels.trec\")\n",
        "\n",
        "# --- Model & Embedding Input Folders ---\n",
        "FINETUNED_RETRIEVER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers\")\n",
        "ADVANCED_FINETUNED_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers_advanced\")\n",
        "EMBEDDINGS_FOLDER = os.path.join(BASE_PATH, \"embeddings_full_comparison\")\n",
        "CROSS_ENCODER_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_cross_encoders\")\n",
        "\n",
        "# --- Output Paths ---\n",
        "RESULTS_CSV_OUTPUT_PATH = os.path.join(BASE_PATH, \"experiment_3_final_pipeline_results.csv\")\n",
        "RESULTS_JSON_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"experiment_3_retrieval_results_json\")\n",
        "os.makedirs(RESULTS_JSON_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- Champion Retriever Configurations (from final retriever evaluation) ---\n",
        "CHAMPION_CONFIGS = [\n",
        "    {\n",
        "        \"name\": \"e5-large-v2_FT_parent_child_cites\",\n",
        "        \"model_key\": \"e5-large-v2_FT\",\n",
        "        \"model_path\": os.path.join(FINETUNED_RETRIEVER_FOLDER, \"e5-large-v2\"),\n",
        "        \"context_key\": \"parent_child_cites\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"e5-large-v2_FT_Advanced_parent\",\n",
        "        \"model_key\": \"e5-large-v2_FT_Advanced\",\n",
        "        \"model_path\": os.path.join(ADVANCED_FINETUNED_FOLDER, \"e5-large-v2\"),\n",
        "        \"context_key\": \"parent\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Cross-Encoder Models to Evaluate ---\n",
        "CROSS_ENCODERS_TO_EVALUATE = {\n",
        "    \"MiniLM_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MiniLM_CrossEncoder\"),\n",
        "    \"MPNet_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MPNet_CrossEncoder\"),\n",
        "    \"MSMarco_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"MSMarco_CrossEncoder\"),\n",
        "    \"BERT_FT\": os.path.join(CROSS_ENCODER_FOLDER, \"BERT_CrossEncoder\"),\n",
        "    \"MiniLM_Pretrained\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    \"MSMarco_Pretrained\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\",\n",
        "    \"BERT_Pretrained\": \"bert-base-uncased\"\n",
        "}\n",
        "\n",
        "# --- Experiment Parameters ---\n",
        "K_INITIAL = 100\n",
        "K_RERANK = 25\n",
        "K_FINAL = 20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Load Static Components ---\n",
        "print(\"Loading static components...\")\n",
        "with open(GRAPH_PATH, \"rb\") as f: G = pickle.load(f)\n",
        "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f: test_data = json.load(f)\n",
        "qid_to_question = {q[\"QuestionID\"]: q[\"Question\"] for q in test_data}\n",
        "print(f\"Loaded {len(test_data)} test questions.\")\n",
        "\n",
        "# Load QRELs\n",
        "qrel = {}\n",
        "with open(QREL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        qid, _, uid, rel = parts[0], parts[1], \" \".join(parts[2:-1]), int(parts[-1])\n",
        "        qrel.setdefault(qid, {})[uid] = rel\n",
        "print(f\"Loaded QRELs for {len(qrel)} queries.\")\n",
        "\n",
        "# Prepare BM25\n",
        "print(\"Preparing BM25 index...\")\n",
        "all_passage_uids = [n for n, d in G.nodes(data=True) if d.get(\"type\") == \"Passage\"]\n",
        "uid_map = {f\"{G.nodes[uid].get('document_id')}|||{G.nodes[uid].get('passage_id')}\": uid for uid in all_passage_uids}\n",
        "corpus_texts = [G.nodes[uid].get(\"text\", \"\") for uid in all_passage_uids]\n",
        "tokenized_corpus = [text.split() for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 ready.\")\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def add_instruction_to_query(query, model_name):\n",
        "    if \"e5\" in model_name or \"bge\" in model_name:\n",
        "        return f\"query: {query}\"\n",
        "    return query\n",
        "\n",
        "def evaluate_run(run_dict, qrel_dict, metrics={\"recall_10\", \"map_cut_10\"}):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict, metrics)\n",
        "    results = evaluator.evaluate(run_dict)\n",
        "    agg = {metric: pytrec_eval.compute_aggregated_measure(metric, [r.get(metric, 0.0) for r in results.values()]) for metric in metrics}\n",
        "    return agg\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    fused = {}\n",
        "    for lst in ranked_lists:\n",
        "        for rank, uid in enumerate(lst):\n",
        "            fused[uid] = fused.get(uid, 0) + 1 / (k + rank + 1)\n",
        "    return sorted(fused.keys(), key=lambda item: fused[item], reverse=True)\n",
        "\n",
        "def format_run_for_json(run_dict, qid_to_question_map, uid_to_internal_uid_map, graph, top_n=10):\n",
        "    output_list = []\n",
        "    for qid, passages in run_dict.items():\n",
        "        sorted_passages = sorted(passages.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "        retrieved_passages_text, retrieved_scores, retrieved_ids = [], [], []\n",
        "        for combined_uid, score in sorted_passages[:top_n]:\n",
        "            internal_uid = uid_to_internal_uid_map.get(combined_uid)\n",
        "            if internal_uid:\n",
        "                retrieved_passages_text.append(graph.nodes[internal_uid].get(\"text\", \"\"))\n",
        "                retrieved_scores.append(score)\n",
        "                retrieved_ids.append(internal_uid)\n",
        "\n",
        "        output_list.append({\n",
        "            \"QuestionID\": qid, \"Question\": qid_to_question_map.get(qid, \"\"),\n",
        "            \"RetrievedPassages\": retrieved_passages_text, \"RetrievedScores\": retrieved_scores,\n",
        "            \"RetrievedIDs\": retrieved_ids\n",
        "        })\n",
        "    return output_list\n",
        "\n",
        "# --- Main Experiment Loop ---\n",
        "all_results = []\n",
        "\n",
        "for config in CHAMPION_CONFIGS:\n",
        "    champion_name = config[\"name\"]\n",
        "    model_path = config[\"model_path\"]\n",
        "    model_key = config[\"model_key\"]\n",
        "    context_key = config[\"context_key\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- TESTING CHAMPION RETRIEVER: {champion_name} ---\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load Embeddings & Query Encoder\n",
        "    print(f\"Loading embeddings from: {EMBEDDINGS_FOLDER}\")\n",
        "    emb_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"embeddings.pkl\")\n",
        "    id_path = os.path.join(EMBEDDINGS_FOLDER, model_key, context_key, \"passage_ids.json\")\n",
        "    try:\n",
        "        with open(emb_path, \"rb\") as f: passage_embeddings = pickle.load(f)\n",
        "        with open(id_path, \"r\") as f: passage_ids = json.load(f)\n",
        "        embeddings_tensor = torch.tensor(passage_embeddings).to(device)\n",
        "        query_encoder = SentenceTransformer(model_path, device=device)\n",
        "        print(\"Champion retriever components loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"FATAL ERROR: Embeddings not found at {emb_path}. Skipping this champion.\")\n",
        "        continue\n",
        "\n",
        "    # Pre-calculate initial retrievals\n",
        "    print(\"Pre-calculating initial hybrid retrievals...\")\n",
        "    initial_retrievals = {}\n",
        "    for q in tqdm(test_data, desc=f\"Initial Retrieval for {champion_name}\"):\n",
        "        qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "        instructed_query = add_instruction_to_query(query, champion_name)\n",
        "        query_emb = query_encoder.encode(instructed_query, convert_to_tensor=True, device=device)\n",
        "        cos_scores = util.pytorch_cos_sim(query_emb, embeddings_tensor)[0]\n",
        "        top_dense = torch.topk(cos_scores, k=min(K_INITIAL, len(passage_ids)))\n",
        "        dense_uids = [passage_ids[idx] for idx in top_dense.indices]\n",
        "\n",
        "        bm25_scores = bm25.get_scores(query.split())\n",
        "        top_bm25 = np.argsort(bm25_scores)[::-1][:K_INITIAL]\n",
        "        bm25_uids = [all_passage_uids[i] for i in top_bm25]\n",
        "\n",
        "        fused_uids = reciprocal_rank_fusion([dense_uids, bm25_uids])\n",
        "        initial_retrievals[qid] = fused_uids[:K_RERANK]\n",
        "\n",
        "    for ce_name, ce_path in CROSS_ENCODERS_TO_EVALUATE.items():\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"--- Evaluating Cross-Encoder: {ce_name} (with {champion_name}) ---\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        try:\n",
        "            ce_tokenizer = AutoTokenizer.from_pretrained(ce_path)\n",
        "            ce_model = AutoModelForSequenceClassification.from_pretrained(ce_path).to(device)\n",
        "            ce_model.eval()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not load model from {ce_path}. Skipping. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "        final_run = {}\n",
        "        for q in tqdm(test_data, desc=f\"Re-ranking with {ce_name}\"):\n",
        "            qid, query = q[\"QuestionID\"], q[\"Question\"]\n",
        "            candidates_uids = initial_retrievals[qid]\n",
        "\n",
        "            ce_input_pairs = [[query, G.nodes[uid].get(\"text\", \"\")] for uid in candidates_uids]\n",
        "\n",
        "            reranked_candidates = []\n",
        "            with torch.no_grad():\n",
        "                inputs = ce_tokenizer(ce_input_pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
        "                logits = ce_model(**inputs).logits\n",
        "\n",
        "                if logits.shape[1] > 1:\n",
        "                    scores = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "                else:\n",
        "                    scores = logits.squeeze().cpu().numpy()\n",
        "\n",
        "            if scores.ndim == 0:\n",
        "                scores = [scores.item()]\n",
        "\n",
        "            for i, uid in enumerate(candidates_uids):\n",
        "                reranked_candidates.append({\"internal_uid\": uid, \"score\": scores[i]})\n",
        "\n",
        "            reranked_candidates = sorted(reranked_candidates, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "            final_run[qid] = {}\n",
        "            for cand in reranked_candidates[:K_FINAL]:\n",
        "                node = G.nodes[cand[\"internal_uid\"]]\n",
        "                combined_uid = f\"{node.get('document_id')}|||{node.get('passage_id')}\"\n",
        "                final_run[qid][combined_uid] = float(cand[\"score\"])\n",
        "\n",
        "        ce_metrics = evaluate_run(final_run, qrel)\n",
        "        all_results.append({\n",
        "            \"Retriever\": champion_name, \"Cross-Encoder\": ce_name, **ce_metrics\n",
        "        })\n",
        "\n",
        "        json_output_path = os.path.join(RESULTS_JSON_OUTPUT_FOLDER, f\"{champion_name}_{ce_name}_results.json\")\n",
        "        json_data = format_run_for_json(final_run, qid_to_question, uid_map, G)\n",
        "        with open(json_output_path, 'w') as f:\n",
        "            json.dump(json_data, f, indent=4)\n",
        "\n",
        "# --- Save and Display Final Results ---\n",
        "df = pd.DataFrame(all_results)\n",
        "# CORRECTED: Rename columns and select only the ones we want\n",
        "df = df.rename(columns={\"recall_10\": \"Recall@10\", \"map_cut_10\": \"MAP@10\"})\n",
        "# Select and reorder columns for the final output\n",
        "final_df = df[[\"Retriever\", \"Cross-Encoder\", \"Recall@10\", \"MAP@10\"]]\n",
        "# CORRECTED: Sort by Recall and MAP\n",
        "final_df = final_df.sort_values(by=[\"Recall@10\", \"MAP@10\"], ascending=False)\n",
        "\n",
        "print(\"\\nüìä Final Evaluation Results:\")\n",
        "print(final_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "final_df.to_csv(RESULTS_CSV_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n‚úÖ CSV summary saved to: {RESULTS_CSV_OUTPUT_PATH}\")\n",
        "print(f\"‚úÖ Detailed JSON results saved to: {RESULTS_JSON_OUTPUT_FOLDER}\")\n",
        "\n",
        "print(\"\\n--- üèÜ Best Performing Full Pipeline ---\")\n",
        "print(final_df.iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Final Evaluation Results:\n",
        "                        Retriever      Cross-Encoder  Recall@10  MAP@10\n",
        "e5-large-v2_FT_parent_child_cites            BERT_FT     0.4580  0.3180\n",
        "e5-large-v2_FT_parent_child_cites         MSMarco_FT     0.4519  0.3345\n",
        "e5-large-v2_FT_parent_child_cites           MPNet_FT     0.4516  0.3543\n",
        "e5-large-v2_FT_parent_child_cites MSMarco_Pretrained     0.4507  0.3248\n",
        "e5-large-v2_FT_parent_child_cites  MiniLM_Pretrained     0.4502  0.3378\n",
        "e5-large-v2_FT_parent_child_cites          MiniLM_FT     0.4499  0.2762\n",
        "   e5-large-v2_FT_Advanced_parent            BERT_FT     0.4434  0.3205\n",
        "   e5-large-v2_FT_Advanced_parent           MPNet_FT     0.4427  0.3478\n",
        "   e5-large-v2_FT_Advanced_parent         MSMarco_FT     0.4407  0.3276\n",
        "   e5-large-v2_FT_Advanced_parent          MiniLM_FT     0.4388  0.3075\n",
        "   e5-large-v2_FT_Advanced_parent  MiniLM_Pretrained     0.4304  0.3242\n",
        "   e5-large-v2_FT_Advanced_parent MSMarco_Pretrained     0.4271  0.3102\n",
        "e5-large-v2_FT_parent_child_cites    BERT_Pretrained     0.4151  0.2053\n",
        "   e5-large-v2_FT_Advanced_parent    BERT_Pretrained     0.1580  0.0315"
      ],
      "metadata": {
        "id": "3lsvMoOTxtqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import up sound alert dependencies\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  #display(Audio(url='https://www.myinstants.com/media/sounds/anime-wow-sound-effect.mp3', autoplay=True))\n",
        "  display(Audio(url='https://www.myinstants.com/media/sounds/money-soundfx.mp3', autoplay=True))\n",
        "## Insert whatever audio file you want above\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "GRSY7uwkdacf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}