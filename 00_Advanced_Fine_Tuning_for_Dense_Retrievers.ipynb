{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1VPdv5c0RhTB2C4sN3IzgTKGgrQVx-LYT",
      "authorship_tag": "ABX9TyNTn9KGmsg6NFB21Nvf4EoB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RegNLP/ContextAware-Regulatory-GraphRAG-ObliQAMP/blob/main/00_Advanced_Fine_Tuning_for_Dense_Retrievers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T27UQuZZqn5E"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Advanced Fine-Tuning for Dense Retriever Models\n",
        "#\n",
        "# Purpose:\n",
        "# 1. Load training and validation datasets.\n",
        "# 2. Generate a high-quality training set by mining \"hard negatives\" using BM25.\n",
        "# 3. Apply instruction-tuning prefixes for compatible models (e.g., e5, bge).\n",
        "# 4. Fine-tune models using OnlineContrastiveLoss with (query, positive, negative)\n",
        "#    triplets.\n",
        "# 5. Use a validation set to save only the best-performing\n",
        "#    model checkpoint and prevent overfitting.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Essential Installations ---\n",
        "!pip install -q -U sentence-transformers transformers datasets rank_bm25\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/RIRAG-MultiPassage-NLLP/\"\n",
        "TRAIN_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_train.json\")\n",
        "DEV_SET_PATH = os.path.join(BASE_PATH, \"QADataset\", \"ObliQA_MultiPassage_val.json\")\n",
        "# Save to a new folder to distinguish from the basic fine-tuned models\n",
        "MODEL_OUTPUT_FOLDER = os.path.join(BASE_PATH, \"fine_tuned_retrievers_advanced\")\n",
        "os.makedirs(MODEL_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Models to Fine-Tune ---\n",
        "BASE_MODELS_TO_FINETUNE = {\n",
        "    #\"e5-large-v2\": \"intfloat/e5-large-v2\",\n",
        "    \"all-mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"bge-base-en-v1.5\": \"BAAI/bge-base-en-v1.5\"\n",
        "}\n",
        "\n",
        "# --- Training Parameters ---\n",
        "NUM_EPOCHS = 3 # Increased epochs, with early stopping to prevent overfitting\n",
        "BATCH_SIZE = 8 # Smaller batch size is often needed for contrastive loss\n",
        "LEARNING_RATE = 1e-5 # Lower learning rate for fine-tuning\n",
        "NUM_HARD_NEGATIVES_PER_POSITIVE = 2 # Mine 2 hard negatives for each positive passage\n",
        "\n",
        "# --- Load Data ---\n",
        "print(\"Loading training and validation data...\")\n",
        "try:\n",
        "    with open(TRAIN_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        train_data = json.load(f)\n",
        "    with open(DEV_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        dev_data = json.load(f)\n",
        "    print(f\"Loaded {len(train_data)} training and {len(dev_data)} validation examples.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"FATAL ERROR: Data file not found: {e}. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# --- Prepare for Hard Negative Mining ---\n",
        "print(\"Preparing for hard negative mining...\")\n",
        "# Create a corpus of all unique passages from the training data\n",
        "all_passages = {p[\"PassageID\"]: p[\"Passage\"] for item in train_data for p in item[\"Passages\"]}\n",
        "corpus_pids = list(all_passages.keys())\n",
        "corpus_texts = [all_passages[pid] for pid in corpus_pids]\n",
        "tokenized_corpus = [text.split(\" \") for text in corpus_texts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(\"BM25 index built for hard negative mining.\")\n",
        "\n",
        "# --- Function for Instruction Prefixes ---\n",
        "def add_instruction(text, model_key, text_type=\"query\"):\n",
        "    if \"e5\" in model_key:\n",
        "        return f\"{text_type}: {text}\"\n",
        "    # BGE models have a specific instruction for queries only\n",
        "    if \"bge\" in model_key and text_type == \"query\":\n",
        "        return f\"Represent this sentence for searching relevant passages: {text}\"\n",
        "    return text\n",
        "\n",
        "# --- Create Training Set with Hard Negatives ---\n",
        "train_samples = []\n",
        "print(\"Mining hard negatives and creating training samples...\")\n",
        "for item in tqdm(train_data, desc=\"Mining negatives\"):\n",
        "    query = item[\"Question\"]\n",
        "    positive_pids = {p[\"PassageID\"] for p in item[\"Passages\"]}\n",
        "\n",
        "    # Find hard negatives for this query\n",
        "    tokenized_query = query.split(\" \")\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "    top_n_indices = np.argsort(bm25_scores)[::-1][:100] # Get top 100 candidates\n",
        "\n",
        "    hard_negatives = []\n",
        "    for idx in top_n_indices:\n",
        "        pid = corpus_pids[idx]\n",
        "        if pid not in positive_pids:\n",
        "            hard_negatives.append(corpus_texts[idx])\n",
        "            if len(hard_negatives) >= NUM_HARD_NEGATIVES_PER_POSITIVE * len(positive_pids):\n",
        "                break\n",
        "\n",
        "    # Create training triplets\n",
        "    for p in item[\"Passages\"]:\n",
        "        positive_passage = p[\"Passage\"]\n",
        "        if hard_negatives:\n",
        "            for neg_passage in hard_negatives:\n",
        "                train_samples.append(InputExample(texts=[query, positive_passage, neg_passage]))\n",
        "        else:\n",
        "            # Fallback if no hard negatives are found\n",
        "            train_samples.append(InputExample(texts=[query, positive_passage, \"\"]))\n",
        "\n",
        "\n",
        "print(f\"Created {len(train_samples)} training triplets.\")\n",
        "\n",
        "# --- Prepare Validation Data ---\n",
        "print(\"Preparing validation data...\")\n",
        "dev_queries = {item[\"QuestionID\"]: item[\"Question\"] for item in dev_data}\n",
        "dev_corpus = {p[\"PassageID\"]: p[\"Passage\"] for item in dev_data for p in item[\"Passages\"]}\n",
        "dev_relevant_docs = {item[\"QuestionID\"]: {p[\"PassageID\"] for p in item[\"Passages\"]} for item in dev_data}\n",
        "print(\"Validation data prepared.\")\n",
        "\n",
        "# --- Main Fine-Tuning Loop ---\n",
        "for model_name, model_path in BASE_MODELS_TO_FINETUNE.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"--- Fine-Tuning Model: {model_name} ---\")\n",
        "    print(f\"Base model: {model_path}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Initialize the SentenceTransformer model\n",
        "    model = SentenceTransformer(model_path)\n",
        "\n",
        "    # 2. Apply instruction prefixes to the training data\n",
        "    instructed_train_samples = []\n",
        "    for sample in train_samples:\n",
        "        instructed_query = add_instruction(sample.texts[0], model_name, \"query\")\n",
        "        instructed_pos = add_instruction(sample.texts[1], model_name, \"passage\")\n",
        "        instructed_neg = add_instruction(sample.texts[2], model_name, \"passage\")\n",
        "        instructed_train_samples.append(InputExample(texts=[instructed_query, instructed_pos, instructed_neg]))\n",
        "\n",
        "    # 3. Create a DataLoader\n",
        "    train_dataloader = DataLoader(instructed_train_samples, shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # 4. Define the loss function for triplet data\n",
        "    train_loss = losses.OnlineContrastiveLoss(model)\n",
        "\n",
        "    # 5. Create the evaluator with instruction prefixes\n",
        "    instructed_dev_queries = {qid: add_instruction(q_text, model_name, \"query\") for qid, q_text in dev_queries.items()}\n",
        "    instructed_dev_corpus = {pid: add_instruction(p_text, model_name, \"passage\") for pid, p_text in dev_corpus.items()}\n",
        "    evaluator = InformationRetrievalEvaluator(\n",
        "        queries=instructed_dev_queries,\n",
        "        corpus=instructed_dev_corpus,\n",
        "        relevant_docs=dev_relevant_docs,\n",
        "        name=f\"{model_name}-val\",\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # 6. Define the output path\n",
        "    output_save_path = os.path.join(MODEL_OUTPUT_FOLDER, model_name)\n",
        "\n",
        "    # 7. Start the training process\n",
        "    model.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        evaluator=evaluator,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        warmup_steps=int(len(train_dataloader) * 0.1),\n",
        "        output_path=output_save_path,\n",
        "        save_best_model=True,\n",
        "        show_progress_bar=True,\n",
        "        evaluation_steps=int(len(train_dataloader) * 0.25) # Evaluate 4 times per epoch\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Best model for '{model_name}' fine-tuned and saved to: {output_save_path}\")\n",
        "\n",
        "print(\"\\nAll dense retriever models have been fine-tuned successfully.\")\n"
      ]
    }
  ]
}